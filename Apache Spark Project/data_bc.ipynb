{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hispanic-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "super-wages",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/notebook\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "twelve-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.pyspark_setup import get_spark_session\n",
    "from ipynb.fs.full.conf_template import Struct as Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "formal-blond",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"data_bc\", swan_spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "pointed-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import random\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "colonial-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "data = [[0, 0, 0.],\n",
    "        [0, 1, 1.],\n",
    "        [1, 0, 1.],\n",
    "        [1, 1, 0.]]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField('a', T.IntegerType(), True),\n",
    "    T.StructField('b', T.IntegerType(), True),\n",
    "    T.StructField('label', T.DoubleType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['a', 'b'], outputCol='features')\n",
    "df = assembler.transform(df)\n",
    "\n",
    "classifier = RandomForestClassifier(numTrees=10, maxDepth=15, labelCol='label', featuresCol='features')\n",
    "model = classifier.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "administrative-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel: uid=RandomForestClassifier_510e8359b8c1, numTrees=10, numClasses=2, numFeatures=2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "guilty-jenny",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1497.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:105)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n\tat org.apache.spark.ml.tree.EnsembleModelReadWrite$.saveImpl(treeModels.scala:464)\n\tat org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelWriter.saveImpl(RandomForestClassifier.scala:406)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 123) (10.233.88.61 executor 1): java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.getWrapped(FileOutputCommitter.java:75)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupTask(FileOutputCommitter.java:167)\n\tat org.apache.hadoop.mapred.OutputCommitter.setupTask(OutputCommitter.java:319)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupTask(HadoopMapReduceCommitProtocol.scala:251)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2255)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.getWrapped(FileOutputCommitter.java:75)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupTask(FileOutputCommitter.java:167)\n\tat org.apache.hadoop.mapred.OutputCommitter.setupTask(OutputCommitter.java:319)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupTask(HadoopMapReduceCommitProtocol.scala:251)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9fc65867c66b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://my-bucket/savedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1497.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:105)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\n\tat org.apache.spark.ml.tree.EnsembleModelReadWrite$.saveImpl(treeModels.scala:464)\n\tat org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelWriter.saveImpl(RandomForestClassifier.scala:406)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 4 times, most recent failure: Lost task 0.3 in stage 11.0 (TID 123) (10.233.88.61 executor 1): java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.getWrapped(FileOutputCommitter.java:75)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupTask(FileOutputCommitter.java:167)\n\tat org.apache.hadoop.mapred.OutputCommitter.setupTask(OutputCommitter.java:319)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupTask(HadoopMapReduceCommitProtocol.scala:251)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2255)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:160)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.<init>(FileOutputCommitter.java:116)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.getWrapped(FileOutputCommitter.java:75)\n\tat org.apache.hadoop.mapred.FileOutputCommitter.setupTask(FileOutputCommitter.java:167)\n\tat org.apache.hadoop.mapred.OutputCommitter.setupTask(OutputCommitter.java:319)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupTask(HadoopMapReduceCommitProtocol.scala:251)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:122)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n\t... 1 more\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save('s3a://my-bucket/savedModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "joint-uruguay",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o285.load.\n: java.lang.UnsupportedOperationException: empty collection\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1465)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1463)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n\tat org.apache.spark.ml.tree.EnsembleModelReadWrite$.loadImpl(treeModels.scala:498)\n\tat org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelReader.load(RandomForestClassifier.scala:420)\n\tat org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelReader.load(RandomForestClassifier.scala:410)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6fdf9babe9fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassificationModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassificationModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/notebook/savedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o285.load.\n: java.lang.UnsupportedOperationException: empty collection\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1465)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1463)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n\tat org.apache.spark.ml.tree.EnsembleModelReadWrite$.loadImpl(treeModels.scala:498)\n\tat org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelReader.load(RandomForestClassifier.scala:420)\n\tat org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelReader.load(RandomForestClassifier.scala:410)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "loaded_model = RandomForestClassificationModel.load('s3://my-bucket/savedModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-african",
   "metadata": {},
   "source": [
    "# Receiving data from BE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improved-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kafka import KafkaConsumer\n",
    "# from json import loads\n",
    "# import json\n",
    "# from decimal import Decimal\n",
    "# import iso8601\n",
    "# from threading import Thread\n",
    "# import pika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chicken-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_shopee = StructType([\n",
    "#     StructField('ordersn', StringType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('shopid', IntegerType(), True),\n",
    "#     StructField('create_time', TimestampType(), True),\n",
    "#     StructField('pay_time', TimestampType(), True),\n",
    "#     StructField('update_time', TimestampType(), True),\n",
    "#     StructField('buyer_username', StringType(), True),\n",
    "#     StructField('recipient_name', StringType(), True),\n",
    "#     StructField('recipient_phone', StringType(), True),\n",
    "#     StructField('recipient_address', StringType(), True),\n",
    "#     StructField('recipient_district', StringType(), True),\n",
    "#     StructField('recipient_city', StringType(), True),\n",
    "#     StructField('recipient_province', StringType(), True),\n",
    "#     StructField('timestamp', TimestampType(), True),\n",
    "#     StructField('estimated_shipping_fee', DecimalType(38,18), True),\n",
    "# ])\n",
    "\n",
    "# schema_bc = StructType([\n",
    "#     StructField('brand', StringType(), True),\n",
    "#     StructField('order_id', LongType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('date_order', TimestampType(), True),\n",
    "#     StructField('customer_id', StringType(), True),\n",
    "#     StructField('payment_method', StringType(), True),\n",
    "#     StructField('date_completed', TimestampType(), True),\n",
    "#     StructField('date_paid', TimestampType(), True),\n",
    "#     StructField('date_invoice', TimestampType(), True),\n",
    "#     StructField('no_invoice', StringType(), True),\n",
    "#     StructField('order_currency', StringType(), True),\n",
    "#     StructField('order_subtotal', DecimalType(38,18), True),\n",
    "#     StructField('cart_discount', DecimalType(38,18), True),\n",
    "#     StructField('redeemed_point', DecimalType(38,18), True),\n",
    "#     StructField('order_shipping', DecimalType(38,18), True),\n",
    "#     StructField('order_total', DecimalType(38,18), True),\n",
    "#     StructField('coupon_usage', StringType(), True),\n",
    "#     StructField('order_stock_reduced', StringType(), True),\n",
    "#     StructField('xendit_invoice', StringType(), True),\n",
    "#     StructField('version', StringType(), True),\n",
    "#     StructField('write_date', TimestampType(), True)\n",
    "# ])\n",
    "\n",
    "# tmp_shopee = []\n",
    "# tmp_bc = []\n",
    "\n",
    "# consumer_bc = KafkaConsumer(\n",
    "#     'com.xyz.bc.transfer',\n",
    "#      bootstrap_servers=['kafka.naufalhilmi.svc.cluster.local:9092'],\n",
    "#      group_id = \"xyz\",\n",
    "# #      consumer_timeout_ms = 60000,\n",
    "#      enable_auto_commit=True,\n",
    "#      value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "# consumer_shopee = KafkaConsumer(\n",
    "#     'com.xyz.shopee.transfer',\n",
    "#      bootstrap_servers=['kafka.naufalhilmi.svc.cluster.local:9092'],\n",
    "#      group_id = \"xyz\",\n",
    "# #      consumer_timeout_ms = 60000,\n",
    "#      enable_auto_commit=True,\n",
    "#      value_deserializer=lambda x: loads(x.decode('utf-8')))\n",
    "\n",
    "# def recieve_bc(consumer):\n",
    "#     for message_bc in consumer:\n",
    "#         message_bc = message_bc.value\n",
    "#         lst_timestmp_bc = ['date_order', 'date_completed', 'date_paid', 'date_invoice', 'write_date']\n",
    "#         lst_decimal_bc = ['order_subtotal', 'cart_discount', 'redeemed_point', 'order_shipping', 'order_total']\n",
    "#         for i in lst_decimal_bc:\n",
    "#             if message_bc[i] == None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 message_bc[i] = Decimal(message_bc[i])\n",
    "            \n",
    "#         for i in lst_timestmp_bc:\n",
    "#             if message_bc[i] == None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 message_bc[i] = iso8601.parse_date(message_bc[i], default_timezone=None)\n",
    "#         tmp_bc.append(message_bc)\n",
    "\n",
    "# def recieve_shopee(consumer):\n",
    "#     for message_shopee in consumer:\n",
    "#         message_shopee = message_shopee.value\n",
    "#         lst_timestmp_shopee = ['create_time', 'pay_time', 'update_time', 'timestamp']\n",
    "#         message_shopee['estimated_shipping_fee'] = Decimal(message_shopee['estimated_shipping_fee'])\n",
    "            \n",
    "#         for i in lst_timestmp_shopee:\n",
    "#             if message_shopee[i] == None:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 message_shopee[i] = iso8601.parse_date(message_shopee[i], default_timezone=None)\n",
    "#         tmp_shopee.append(message_shopee)\n",
    "\n",
    "# threads = list()\n",
    "        \n",
    "# bc_thread = Thread(target=recieve_bc, args=(consumer_bc,))\n",
    "# shopee_thread = Thread(target=recieve_shopee, args=(consumer_shopee,))\n",
    "\n",
    "# threads.append(bc_thread)\n",
    "# threads.append(shopee_thread)\n",
    "\n",
    "# for i in threads:\n",
    "#     i.start()\n",
    "\n",
    "# for i in threads:\n",
    "#     i.join()\n",
    "\n",
    "# df_recieve_bc = spark.createDataFrame(tmp_bc,schema_bc)        \n",
    "# df_recieve_shopee = spark.createDataFrame(tmp_shopee,schema_shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "oriental-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_shopee = StructType([\n",
    "#     StructField('ordersn', StringType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('shopid', IntegerType(), True),\n",
    "#     StructField('create_time', TimestampType(), True),\n",
    "#     StructField('pay_time', TimestampType(), True),\n",
    "#     StructField('update_time', TimestampType(), True),\n",
    "#     StructField('buyer_username', StringType(), True),\n",
    "#     StructField('recipient_name', StringType(), True),\n",
    "#     StructField('recipient_phone', StringType(), True),\n",
    "#     StructField('recipient_address', StringType(), True),\n",
    "#     StructField('recipient_district', StringType(), True),\n",
    "#     StructField('recipient_city', StringType(), True),\n",
    "#     StructField('recipient_province', StringType(), True),\n",
    "#     StructField('timestamp', TimestampType(), True),\n",
    "#     StructField('estimated_shipping_fee', DecimalType(38,18), True),\n",
    "# ])\n",
    "\n",
    "# schema_bc = StructType([\n",
    "#     StructField('brand', StringType(), True),\n",
    "#     StructField('order_id', LongType(), True),\n",
    "#     StructField('order_status', StringType(), True),\n",
    "#     StructField('date_order', TimestampType(), True),\n",
    "#     StructField('customer_id', StringType(), True),\n",
    "#     StructField('payment_method', StringType(), True),\n",
    "#     StructField('date_completed', TimestampType(), True),\n",
    "#     StructField('date_paid', TimestampType(), True),\n",
    "#     StructField('date_invoice', TimestampType(), True),\n",
    "#     StructField('no_invoice', StringType(), True),\n",
    "#     StructField('order_currency', StringType(), True),\n",
    "#     StructField('order_subtotal', DecimalType(38,18), True),\n",
    "#     StructField('cart_discount', DecimalType(38,18), True),\n",
    "#     StructField('redeemed_point', DecimalType(38,18), True),\n",
    "#     StructField('order_shipping', DecimalType(38,18), True),\n",
    "#     StructField('order_total', DecimalType(38,18), True),\n",
    "#     StructField('coupon_usage', StringType(), True),\n",
    "#     StructField('order_stock_reduced', StringType(), True),\n",
    "#     StructField('xendit_invoice', StringType(), True),\n",
    "#     StructField('version', StringType(), True),\n",
    "#     StructField('write_date', TimestampType(), True)\n",
    "# ])\n",
    "\n",
    "# tmp_shopee = []\n",
    "# tmp_bc = []\n",
    "\n",
    "# class ThreadedConsumer(Thread):\n",
    "#     def __init__(self, RABBIT_URL, QUEUE_NAME):\n",
    "#         Thread.__init__(self)\n",
    "#         self.queue_name = QUEUE_NAME\n",
    "#         parameters = pika.URLParameters(RABBIT_URL)\n",
    "#         connection = pika.BlockingConnection(parameters)\n",
    "#         self.channel = connection.channel()\n",
    "#         # self.channel.queue_declare(queue=QUEUE_NAME, auto_delete=False)\n",
    "#         # self.channel.queue_bind(queue=QUEUE_NAME, exchange=EXCHANGE, routing_key=ROUTING_KEY)\n",
    "#         # self.channel.basic_qos(prefetch_count=THREADS*10)\n",
    "#         self.channel.basic_consume(QUEUE_NAME, on_message_callback=self.callback)\n",
    "#         Thread(target=self.channel.basic_consume(QUEUE_NAME, on_message_callback=self.callback))\n",
    "        \n",
    "#     def callback(self, channel, method, properties, body):\n",
    "#         message = json.loads(body)\n",
    "#         if self.queue_name == \"queue.brandcommerce\" :\n",
    "#             lst_timestmp_bc = ['date_order', 'date_completed', 'date_paid', 'date_invoice', 'write_date']\n",
    "#             lst_decimal_bc = ['order_subtotal', 'cart_discount', 'redeemed_point', 'order_shipping', 'order_total']\n",
    "#             for i in lst_decimal_bc:\n",
    "#                 if message[i] == None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     message[i] = Decimal(message[i])\n",
    "\n",
    "#             for i in lst_timestmp_bc:\n",
    "#                 if message[i] == None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     message[i] = iso8601.parse_date(message[i], default_timezone=None)\n",
    "#             tmp_bc.append(message)\n",
    "#             channel.basic_ack(delivery_tag=method.delivery_tag)\n",
    "#         else:\n",
    "#             lst_timestmp_shopee = ['create_time', 'pay_time', 'update_time', 'timestamp']\n",
    "#             message['estimated_shipping_fee'] = Decimal(message['estimated_shipping_fee'])\n",
    "\n",
    "#             for i in lst_timestmp_shopee:\n",
    "#                 if message[i] == None:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     message[i] = iso8601.parse_date(message[i], default_timezone=None)\n",
    "#             tmp_shopee.append(message)\n",
    "#             channel.basic_ack(delivery_tag=method.delivery_tag)\n",
    "\n",
    "#     def run(self):\n",
    "#         print ('starting thread to consume from rabbit...')\n",
    "#         self.channel.start_consuming()\n",
    "\n",
    "# thread_list = []\n",
    "# thread_list.append(ThreadedConsumer('amqp://user:user@rabbitmq-0.rabbitmq-headless.naufalhilmi.svc.cluster.local:5672', \"queue.brandcommerce\"))\n",
    "# thread_list.append(ThreadedConsumer('amqp://user:user@rabbitmq-1.rabbitmq-headless.naufalhilmi.svc.cluster.local:5672', \"queue.shopee\"))\n",
    "# for i in thread_list:\n",
    "#     i.start()\n",
    "\n",
    "# for i in thread_list:\n",
    "#     i.join()\n",
    "\n",
    "# df_recieve_bc = spark.createDataFrame(tmp_bc,schema_bc)        \n",
    "# df_recieve_shopee = spark.createDataFrame(tmp_shopee,schema_shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continental-player",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_recieve_bc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "limiting-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_recieve_shopee.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "crude-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_recieve_bc.write.format(\"jdbc\")\\\n",
    "# .option(\"url\",'jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres')\\\n",
    "# .option(\"dbtable\",'brand_commerce.order_transaction_bc_naufal')\\\n",
    "# .option(\"user\",'postgres')\\\n",
    "# .option(\"password\",'postgres')\\\n",
    "# .option(\"driver\",\"org.postgresql.Driver\")\\\n",
    "# .mode('append').save()\n",
    "\n",
    "# df_recieve_shopee.write.format(\"jdbc\")\\\n",
    "# .option(\"url\",'jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres')\\\n",
    "# .option(\"dbtable\",'shopee.order_transaction_shopee_naufal')\\\n",
    "# .option(\"user\",'postgres')\\\n",
    "# .option(\"password\",'postgres')\\\n",
    "# .option(\"driver\",\"org.postgresql.Driver\")\\\n",
    "# .mode('append').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-minute",
   "metadata": {},
   "source": [
    "# Postgres Data Brand_Commerce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cutting-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_bc_1 = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='brand_commerce.users',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "right-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_bc_2 = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='public.users',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "auburn-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shipping_bc = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='brand_commerce.order_shipping',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distant-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product_bc = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='brand_commerce.order_product',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "passive-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_bc = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='brand_commerce.order_transaction_bc_naufal',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-lewis",
   "metadata": {},
   "source": [
    "# Postgres Data Shopee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "terminal-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shopee = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='shopee.order_transaction_shopee_naufal',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "supposed-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_product_shopee = spark.read.format(\"jdbc\").\\\n",
    "options(\n",
    "         url='jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres',\n",
    "         dbtable='shopee.income_item_detail',\n",
    "         user='postgres',\n",
    "         password='postgres',\n",
    "         driver='org.postgresql.Driver').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fifth-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_bc = ['wc-closed', 'accepted', 'Completed', 'completed', 'wc-completed', 'Waiting Payment', 'Under Shipment']\n",
    "lst_shopee = ['READY_TO_SHIP', 'COMPLETED', 'SHIPPED', 'TO_CONFIRM_RECEIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "instant-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_brand_bc = ['Emina', 'Wardah', 'Kahf']\n",
    "lst_brand_shopee = [59763733, 63983008, 326487418]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "young-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_bc = order_bc.filter(col('order_status').isin(lst_bc))\n",
    "order_bc = order_bc.filter(col(\"date_paid\").isNotNull())\n",
    "order_bc = order_bc.filter(col('brand').isin(lst_brand_bc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deadly-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shopee = order_shopee.filter(col('order_status').isin(lst_shopee))\n",
    "order_shopee = order_shopee.filter(col(\"pay_time\").isNotNull())\n",
    "order_shopee = order_shopee.filter(col('shopid').isin(lst_brand_shopee))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-spider",
   "metadata": {},
   "source": [
    "# Preprocessing raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "automotive-prince",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Update order_id in order_billing data to match the brand\n",
    "# order_shipping_bc = order_shipping_bc\\\n",
    "#       .withColumn(\"shipping_province\", when(upper(col('shipping_province')).contains('JAWA BARAT'),'Jawa Barat') \\\n",
    "#       .when(upper(col('shipping_province')).contains('DKI JAKARTA'),'DKI Jakarta') \\\n",
    "#       .when(upper(col('shipping_province')).contains('BANTEN'),'Banten') \\\n",
    "#       .when(upper(col('shipping_province')).contains('JAWA TENGAH'),'Jawa Tengah') \\\n",
    "#       .when(upper(col('shipping_province')).contains('JAWA TIMUR'),'Jawa Timur') \\\n",
    "#       .when(upper(col('shipping_province')).contains('SUMATERA UTARA'),'Sumatera Utara') \\\n",
    "#       .otherwise('Other'))\\\n",
    "#       .withColumn(\"order_id\", when(order_shipping_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "#       .when(order_shipping_bc.brand == \"Make Over\",concat(col('order_id'), lit(\"M\"))) \\\n",
    "#       .when(order_shipping_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "#       .when(order_shipping_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "#       .when(order_shipping_bc.brand == \"Crystallure\",concat(col('order_id'), lit(\"C\"))) \\\n",
    "#       .otherwise(order_shipping_bc.order_id))\n",
    "    \n",
    "# order_product_bc = order_product_bc\\\n",
    "#       .withColumn(\"order_id\", when(order_product_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Make Over\",concat(col('order_id'), lit(\"M\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Crystallure\",concat(col('order_id'), lit(\"C\"))) \\\n",
    "#       .otherwise(order_product_bc.order_id))\\\n",
    "#       .withColumn(\"order_item_id\", when(order_product_bc.brand == \"Emina\",concat(col('order_item_id'), lit(\"E\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Make Over\",concat(col('order_item_id'), lit(\"M\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Wardah\",concat(col('order_item_id'), lit(\"W\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Kahf\",concat(col('order_item_id'), lit(\"K\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Crystallure\",concat(col('order_item_id'), lit(\"C\"))) \\\n",
    "#       .otherwise(order_product_bc.order_item_id))\\\n",
    "#       .withColumn(\"product_id\", when(order_product_bc.brand == \"Emina\",concat(col('product_id'), lit(\"E\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Make Over\",concat(col('product_id'), lit(\"M\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Wardah\",concat(col('product_id'), lit(\"W\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Kahf\",concat(col('product_id'), lit(\"K\"))) \\\n",
    "#       .when(order_product_bc.brand == \"Crystallure\",concat(col('product_id'), lit(\"C\"))) \\\n",
    "#       .otherwise(order_product_bc.product_id))\n",
    "\n",
    "# order_bc = order_bc\\\n",
    "#       .withColumn(\"customer_id\", when(order_bc.brand == \"Emina\",concat(col('customer_id'), lit(\"-E-bc\"))) \\\n",
    "#       .when(order_bc.brand == \"Make Over\",concat(col('customer_id'), lit(\"-M-bc\"))) \\\n",
    "#       .when(order_bc.brand == \"Wardah\",concat(col('customer_id'), lit(\"-W-bc\"))) \\\n",
    "#       .when(order_bc.brand == \"Kahf\",concat(col('customer_id'), lit(\"-K-bc\"))) \\\n",
    "#       .when(order_bc.brand == \"Crystallure\",concat(col('customer_id'), lit(\"-C-bc\"))) \\\n",
    "#       .otherwise(order_bc.customer_id))\\\n",
    "#       .withColumn(\"order_id\", when(order_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "#       .when(order_bc.brand == \"Make Over\",concat(col('order_id'), lit(\"M\"))) \\\n",
    "#       .when(order_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "#       .when(order_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "#       .when(order_bc.brand == \"Crystallure\",concat(col('order_id'), lit(\"C\"))) \\\n",
    "#       .otherwise(order_bc.order_id))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "answering-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order_shopee = order_shopee\\\n",
    "#       .withColumn(\"recipient_province\", when(upper(col('recipient_province')).contains('JAWA BARAT'),'Jawa Barat') \\\n",
    "#       .when(upper(col('recipient_province')).contains('DKI JAKARTA'),'DKI Jakarta') \\\n",
    "#       .when(upper(col('recipient_province')).contains('BANTEN'),'Banten') \\\n",
    "#       .when(upper(col('recipient_province')).contains('JAWA TENGAH'),'Jawa Tengah') \\\n",
    "#       .when(upper(col('recipient_province')).contains('JAWA TIMUR'),'Jawa Timur') \\\n",
    "#       .when(upper(col('recipient_province')).contains('SUMATERA UTARA'),'Sumatera Utara') \\\n",
    "#       .otherwise('Other'))\\\n",
    "#       .withColumn(\"buyer_username\", when(order_shopee.shopid == 59763733, concat(col('buyer_username'), lit(\"-W-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 575053680, concat(col('buyer_username'), lit(\"-B-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 637555425, concat(col('buyer_username'), lit(\"-T-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 63983008, concat(col('buyer_username'), lit(\"-E-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 326487418, concat(col('buyer_username'), lit(\"-K-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 524963178, concat(col('buyer_username'), lit(\"-L-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 63984475, concat(col('buyer_username'), lit(\"-M-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 401724234, concat(col('buyer_username'), lit(\"-O-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 652866307, concat(col('buyer_username'), lit(\"-I-shopee\"))) \\\n",
    "#       .when(order_shopee.shopid == 625116419, concat(col('buyer_username'), lit(\"-C-shopee\"))) \\\n",
    "#       .otherwise(order_shopee.shopid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "younger-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update order_id in order_billing data to match the brand\n",
    "user_bc_1 = user_bc_1.select('brand','user_id','nickname')\n",
    "user_bc_2 = user_bc_2.select('brand','user_id','nickname')\n",
    "user_bc = user_bc_1.union(user_bc_2)\n",
    "\n",
    "user_bc = user_bc\\\n",
    "      .withColumn(\"user_id\", when(user_bc.brand == \"Emina\",concat(col('user_id'), lit(\"E\"))) \\\n",
    "      .when(user_bc.brand == \"Wardah\",concat(col('user_id'), lit(\"W\"))) \\\n",
    "      .when(user_bc.brand == \"Kahf\",concat(col('user_id'), lit(\"K\"))) \\\n",
    "      .otherwise(user_bc_1.user_id))\n",
    "\n",
    "order_shipping_bc_2 = order_shipping_bc\\\n",
    "      .withColumn(\"order_id\", when(order_shipping_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "      .when(order_shipping_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "      .when(order_shipping_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "      .otherwise(order_shipping_bc.order_id))\n",
    "\n",
    "order_shipping_bc = order_shipping_bc_2\\\n",
    "      .withColumn(\"shipping_province\", when(upper(col('shipping_province')).contains('JAWA BARAT'),'Jawa Barat') \\\n",
    "      .when(upper(col('shipping_province')).contains('DKI JAKARTA'),'DKI Jakarta') \\\n",
    "      .when(upper(col('shipping_province')).contains('BANTEN'),'Banten') \\\n",
    "      .when(upper(col('shipping_province')).contains('JAWA TENGAH'),'Jawa Tengah') \\\n",
    "      .when(upper(col('shipping_province')).contains('JAWA TIMUR'),'Jawa Timur') \\\n",
    "      .when(upper(col('shipping_province')).contains('SUMATERA UTARA'),'Sumatera Utara') \\\n",
    "      .otherwise('Other'))\n",
    "    \n",
    "order_product_bc = order_product_bc\\\n",
    "      .withColumn(\"order_id\", when(order_product_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "      .when(order_product_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "      .when(order_product_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "      .otherwise(order_product_bc.order_id))\\\n",
    "      .withColumn(\"order_item_id\", when(order_product_bc.brand == \"Emina\",concat(col('order_item_id'), lit(\"E\"))) \\\n",
    "      .when(order_product_bc.brand == \"Wardah\",concat(col('order_item_id'), lit(\"W\"))) \\\n",
    "      .when(order_product_bc.brand == \"Kahf\",concat(col('order_item_id'), lit(\"K\"))) \\\n",
    "      .otherwise(order_product_bc.order_item_id))\\\n",
    "      .withColumn(\"product_id\", when(order_product_bc.brand == \"Emina\",concat(col('product_id'), lit(\"E\"))) \\\n",
    "      .when(order_product_bc.brand == \"Wardah\",concat(col('product_id'), lit(\"W\"))) \\\n",
    "      .when(order_product_bc.brand == \"Kahf\",concat(col('product_id'), lit(\"K\"))) \\\n",
    "      .otherwise(order_product_bc.product_id))\n",
    "\n",
    "order_bc = order_bc\\\n",
    "      .withColumn(\"customer_id\", when(order_bc.brand == \"Emina\",concat(col('customer_id'), lit(\"E\"))) \\\n",
    "      .when(order_bc.brand == \"Wardah\",concat(col('customer_id'), lit(\"W\"))) \\\n",
    "      .when(order_bc.brand == \"Kahf\",concat(col('customer_id'), lit(\"K\"))) \\\n",
    "      .otherwise(order_bc.customer_id))\\\n",
    "      .withColumn(\"order_id\", when(order_bc.brand == \"Emina\",concat(col('order_id'), lit(\"E\"))) \\\n",
    "      .when(order_bc.brand == \"Wardah\",concat(col('order_id'), lit(\"W\"))) \\\n",
    "      .when(order_bc.brand == \"Kahf\",concat(col('order_id'), lit(\"K\"))) \\\n",
    "      .otherwise(order_bc.order_id))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "serial-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_shopee_2 = order_shopee\n",
    "\n",
    "order_shopee = order_shopee_2\\\n",
    "      .withColumn(\"recipient_province\", when(upper(col('recipient_province')).contains('JAWA BARAT'),'Jawa Barat') \\\n",
    "      .when(upper(col('recipient_province')).contains('DKI JAKARTA'),'DKI Jakarta') \\\n",
    "      .when(upper(col('recipient_province')).contains('BANTEN'),'Banten') \\\n",
    "      .when(upper(col('recipient_province')).contains('JAWA TENGAH'),'Jawa Tengah') \\\n",
    "      .when(upper(col('recipient_province')).contains('JAWA TIMUR'),'Jawa Timur') \\\n",
    "      .when(upper(col('recipient_province')).contains('SUMATERA UTARA'),'Sumatera Utara') \\\n",
    "      .otherwise('Other'))\n",
    "\n",
    "order_shopee = order_shopee\\\n",
    "      .withColumn(\"shopid\", when(order_shopee.shopid == 59763733, 'Wardah') \\\n",
    "      .when(order_shopee.shopid == 63983008, 'Emina') \\\n",
    "      .when(order_shopee.shopid == 326487418, 'Kahf') \\\n",
    "      .otherwise(order_shopee.shopid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-access",
   "metadata": {},
   "source": [
    "# Extract feature for ML for Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "productive-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee():\n",
    "    order_shopee_tmp = order_shopee.select('ordersn', 'create_time', 'buyer_username', 'recipient_province','shopid')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    \n",
    "    max_extract_month = predict_month + relativedelta(months=-1)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') < predict_month) & \n",
    "                                                   (trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    qty_order = order_product_shopee.groupBy('ordersn').agg(sum('original_price').alias('revenue'), sum('qty_purchased').alias('qty'))\\\n",
    "    .select('ordersn','revenue','qty')\n",
    "    \n",
    "    days_diff = order_shopee_extract.select('create_time', 'buyer_username')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('create_time')))\n",
    "    w = Window().partitionBy('buyer_username').orderBy(['buyer_username', 'create_time'])\n",
    "    days_diff = days_diff.dropDuplicates([\"buyer_username\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"buyer_username\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(qty_order, ['ordersn'])\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('count_order', lit(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('max_extract_month', lit(max_extract_month))\n",
    "    \n",
    "    final_data = order_shopee_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),max('recipient_province').alias('dc_code'),sum('revenue').alias('revenue'),\n",
    "         max('create_time').alias('max_tgl'),max('max_extract_month').alias('max_extract_month'),\n",
    "         size(collect_set('shopid')).alias('brand'),sum('qty').alias('qty'))\\\n",
    "    .withColumn('recency', datediff(last_day(col('max_extract_month')), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code','qty','brand')\n",
    "    \n",
    "    days_diff = days_diff.withColumnRenamed('buyer_username','customer_id')\n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "varied-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc():\n",
    "    # Slicing data agar 6 bulan saja\n",
    "    order_bc_tmp = order_bc.select('order_id', 'date_order', 'customer_id','brand')\n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    max_extract_month = predict_month + relativedelta(months=-1)\n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') < predict_month) & \n",
    "                                           (trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    # Ekstrak fitur quantity, days_diff, dc_code, dan count_order\n",
    "    qty_order = order_product_bc.groupBy('order_id').agg(sum('line_subtotal').alias('revenue'), sum('qty').alias('qty'))\\\n",
    "    .select('order_id', 'revenue','qty')\n",
    "\n",
    "    dc_user = order_shipping_bc.select('order_id', 'shipping_province')\n",
    "    \n",
    "    days_diff = order_bc_extract.select('date_order', 'customer_id')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('date_order')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_order'])\n",
    "    days_diff = days_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"customer_id\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(qty_order, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.join(dc_user, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.withColumn('count_order', lit(1))\n",
    "    order_bc_extract = order_bc_extract.withColumn('max_extract_month', lit(max_extract_month))\n",
    "    \n",
    "    final_data = order_bc_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),sum('revenue').alias('revenue'), max('shipping_province').alias('dc_code'),\n",
    "         max('date_order').alias('max_tgl'),max('max_extract_month').alias('max_extract_month'),\n",
    "         size(collect_set('brand')).alias('brand'), sum('qty').alias('qty'))\\\n",
    "    .withColumn('recency', datediff(last_day(col('max_extract_month')), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code','qty','brand')\n",
    "    \n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cellular-miller",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data_bc = transform_data_bc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "developmental-functionality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data_shopee = transform_data_shopee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "homeless-strength",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_data = final_data_shopee.union(final_data_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "defined-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_median = final_data.select(percentile_approx('frequency',0.5)).take(1)[0][0]\n",
    "recency_median = final_data.select(percentile_approx('recency',0.5)).take(1)[0][0]\n",
    "revenue_median = final_data.select(percentile_approx('revenue',0.5)).take(1)[0][0]\n",
    "\n",
    "final_data = final_data.withColumn('frequency_median',lit(frequency_median))\n",
    "final_data = final_data.withColumn('recency_median',lit(recency_median))\n",
    "final_data = final_data.withColumn('revenue_median',lit(revenue_median))\n",
    "\n",
    "final_data = final_data.withColumn('label', when((col('frequency')>col('frequency_median'))&\n",
    "                                                (col('recency')<col('recency_median'))&\n",
    "                                                (col('revenue')>col('revenue_median')),1).otherwise(0))\n",
    "\n",
    "final_data = final_data.drop('frequency_median','recency_median','revenue_median','revenue','recency','frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "latin-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = final_data.select('customer_id','frequency','revenue','recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "appointed-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# va = VectorAssembler(outputCol='features_frequency')\n",
    "# va.setInputCols(['frequency'])\n",
    "# combine_data = va.transform(a)\n",
    "\n",
    "# va = VectorAssembler(outputCol='features_revenue')\n",
    "# va.setInputCols(['revenue'])\n",
    "# combine_data = va.transform(combine_data)\n",
    "\n",
    "# va = VectorAssembler(outputCol='features_recency')\n",
    "# va.setInputCols(['recency'])\n",
    "# combine_data = va.transform(combine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bottom-recycling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "shaped-instrument",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.clustering import KMeans\n",
    "# from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# silhouette_score=[]\n",
    "# evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='features_recency', \\\n",
    "#                                 metricName='silhouette', distanceMeasure='squaredEuclidean')\n",
    "# for i in range(2,10):\n",
    "    \n",
    "#     KMeans_algo=KMeans(featuresCol='features_recency', k=i)\n",
    "    \n",
    "#     KMeans_fit=KMeans_algo.fit(combine_data)\n",
    "    \n",
    "#     output=KMeans_fit.transform(combine_data)\n",
    "    \n",
    "#\n",
    "    \n",
    "#     score=evaluator.evaluate(output)\n",
    "    \n",
    "#     silhouette_score.append(score)\n",
    "    \n",
    "#     print(\"Silhouette Score:\",score,' K:',i)\n",
    "    \n",
    "# #Visualizing the silhouette scores in a plot\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "# ax.plot(range(2,10),silhouette_score)\n",
    "# ax.set_xlabel('k')\n",
    "# ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cathedral-poverty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KMeans_algo=KMeans(featuresCol='features_frequency', k=7, predictionCol='cluster_frequency')\n",
    "# KMeans_fit=KMeans_algo.fit(combine_data)\n",
    "# output=KMeans_fit.transform(combine_data)\n",
    "\n",
    "# KMeans_algo=KMeans(featuresCol='features_revenue', k=6, predictionCol='cluster_revenue')\n",
    "# KMeans_fit=KMeans_algo.fit(output)\n",
    "# output=KMeans_fit.transform(output)\n",
    "\n",
    "# KMeans_algo=KMeans(featuresCol='features_recency', k=4, predictionCol='cluster_recency')\n",
    "# KMeans_fit=KMeans_algo.fit(output)\n",
    "# output=KMeans_fit.transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "unnecessary-concrete",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "graduate-idaho",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# output = output.drop('features_frequency', 'features_revenue', 'features_recency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "compound-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data = final_data.join(output,['customer_id','frequency','revenue','recency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "alien-islam",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, dc_code: string, qty: bigint, brand: int, days_diff: int, label: int]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-passion",
   "metadata": {},
   "source": [
    "##  ML Proses Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "spiritual-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "changing-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature(data):\n",
    "    indexer_dc_code = StringIndexer(inputCol='dc_code', outputCol='dc_code_num')\n",
    "    index_data_dc_code = indexer_dc_code.fit(data).transform(data)\n",
    "    encoder = OneHotEncoder(inputCol='dc_code_num', outputCol = 'dc_code_vec')\n",
    "    onehotdata = encoder.fit(index_data_dc_code).transform(index_data_dc_code)\n",
    "    \n",
    "#     encoder = OneHotEncoder(inputCol='cluster_frequency', outputCol = 'cluster_frequency_vec')\n",
    "#     onehotdata = encoder.fit(onehotdata).transform(onehotdata)\n",
    "#     encoder = OneHotEncoder(inputCol='cluster_revenue', outputCol = 'cluster_revenue_vec')\n",
    "#     onehotdata = encoder.fit(onehotdata).transform(onehotdata)\n",
    "#     encoder = OneHotEncoder(inputCol='cluster_recency', outputCol = 'cluster_recency_vec')\n",
    "#     onehotdata = encoder.fit(onehotdata).transform(onehotdata)\n",
    "    \n",
    "    va = VectorAssembler(outputCol='features')\n",
    "    va.setInputCols(['days_diff','dc_code_vec','qty','brand'])\n",
    "    combine_data = va.transform(onehotdata).select(['features','label'])\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ethical-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oversampling_data(train_data, potentialDf_count, nonPotentialDf_count):\n",
    "    potentialDf = train_data.filter(\"label=1.0\")\n",
    "    nonPotentialDf = train_data.filter(\"label=0.0\")\n",
    "    sampleRatio = nonPotentialDf_count / potentialDf_count\n",
    "    a = range(int(sampleRatio))\n",
    "    oversampled_df = potentialDf.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "    train_df = nonPotentialDf.unionAll(oversampled_df)\n",
    "    return train_df\n",
    "\n",
    "def make_undersampling_data(train_data, potentialDf_count, nonPotentialDf_count):\n",
    "    potentialDf = train_data.filter(\"label=1.0\")\n",
    "    nonPotentialDf= train_data.filter(\"label=0.0\")\n",
    "    sampleRatio = potentialDf_count / nonPotentialDf_count\n",
    "    nonPotentialSampleDf = nonPotentialDf.sample(False, sampleRatio)\n",
    "    train_df = potentialDf.unionAll(nonPotentialSampleDf)\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "polish-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_norm_1(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: 1, normalizer.outputCol:\"norm_1\"})\n",
    "    return data.select('label',\"norm_1\")\n",
    "\n",
    "def transform_data_norm_inf(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: float(\"inf\"), normalizer.outputCol:\"norm_inf\"})\n",
    "    return data.select('label',\"norm_inf\")\n",
    "\n",
    "def transform_data_standard_scaler(data):\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    scalerModel = scaler.fit(data)\n",
    "    data = scalerModel.transform(data)\n",
    "    return data.select('label',\"scaled_features\")\n",
    "\n",
    "def transform_data_minmax_scaler(data):\n",
    "    mmscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"mmscaled_features\")\n",
    "    mmscalerModel = mmscaler.fit(data)\n",
    "    data = mmscalerModel.transform(data)\n",
    "    return data.select('label',\"mmscaled_features\")\n",
    "\n",
    "def transform_data_maxabs_scaler(data):\n",
    "    mascaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"mascaled_features\")\n",
    "    mascalerModel = mascaler.fit(data)\n",
    "    data = mascalerModel.transform(data)\n",
    "    return data.select('label',\"mascaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "potential-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsvc_train_tuning_model(train_data, features):\n",
    "    model = LinearSVC(labelCol=\"label\", maxIter=10, featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def lr_train_tuning_model(train_data, features):\n",
    "    model = LogisticRegression(labelCol=\"label\", maxIter=10, featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def rf_train_tuning_model(train_data, features):\n",
    "    model = RandomForestClassifier(labelCol=\"label\", featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def gbt_train_tuning_model(train_data, features):\n",
    "    model = GBTClassifier(labelCol=\"label\", maxIter=10, featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model\n",
    "\n",
    "def dt_train_tuning_model(train_data, features):\n",
    "    model = DecisionTreeClassifier(labelCol=\"label\", featuresCol=features)\n",
    "    trained_model = model.fit(train_data)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "convenient-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_model_f1(pred):\n",
    "    f1 = evaluator.evaluate(pred,{evaluator.metricName: \"fMeasureByLabel\", evaluator.metricLabel: 1.0})  \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "needed-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_model_precision(pred):\n",
    "    precision = evaluator.evaluate(pred,{evaluator.metricName: \"precisionByLabel\", evaluator.metricLabel: 1.0}) \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "operating-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_model_recall(pred):\n",
    "    recall = evaluator.evaluate(pred,{evaluator.metricName: \"recallByLabel\", evaluator.metricLabel: 1.0})\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "rental-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_model_accuracy(pred):\n",
    "    accuracy = evaluator.evaluate(pred,{evaluator.metricName: \"accuracy\"})\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "coral-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def metric_model_f1_multi(y_true, y_pred):\n",
    "    f1_multi = f1_score(y_true, y_pred, average='macro')\n",
    "    return f1_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-designer",
   "metadata": {},
   "source": [
    "## Prepare data for ML Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "welsh-pocket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combine_data = combine_feature(final_data)\n",
    "raw_data_train, raw_data_test = combine_data.randomSplit([0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dental-stone",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "potential_count = raw_data_train.filter(\"label=1.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "excited-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonPotential_count = raw_data_train.filter(\"label=0.0\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "level-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train_over = make_oversampling_data(raw_data_train,potential_count,nonPotential_count)\n",
    "\n",
    "# data_original_train = data_train_over\n",
    "# data_original_test = raw_data_test\n",
    "\n",
    "# data_norm_1_train = transform_data_norm_1(data_train_over)\n",
    "# data_norm_1_test = transform_data_norm_1(raw_data_test)\n",
    "\n",
    "# data_norm_inf_train = transform_data_norm_inf(data_train_over)\n",
    "# data_norm_inf_test = transform_data_norm_inf(raw_data_test)\n",
    "\n",
    "# data_standard_scaler_train = transform_data_standard_scaler(data_train_over)\n",
    "# data_standard_scaler_test = transform_data_standard_scaler(raw_data_test)\n",
    "\n",
    "# data_minmax_scaler_train = transform_data_minmax_scaler(data_train_over)\n",
    "# data_minmax_scaler_test = transform_data_minmax_scaler(raw_data_test)\n",
    "\n",
    "# data_maxabs_scaler_train = transform_data_maxabs_scaler(data_train_over)\n",
    "# data_maxabs_scaler_test = transform_data_maxabs_scaler(raw_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dressed-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_under = make_undersampling_data(raw_data_train,potential_count,nonPotential_count)\n",
    "\n",
    "data_original_train = data_train_under\n",
    "data_original_test = raw_data_test\n",
    "\n",
    "data_norm_1_train = transform_data_norm_1(data_train_under)\n",
    "data_norm_1_test = transform_data_norm_1(raw_data_test)\n",
    "\n",
    "data_norm_inf_train = transform_data_norm_inf(data_train_under)\n",
    "data_norm_inf_test = transform_data_norm_inf(raw_data_test)\n",
    "\n",
    "data_standard_scaler_train = transform_data_standard_scaler(data_train_under)\n",
    "data_standard_scaler_test = transform_data_standard_scaler(raw_data_test)\n",
    "\n",
    "data_minmax_scaler_train = transform_data_minmax_scaler(data_train_under)\n",
    "data_minmax_scaler_test = transform_data_minmax_scaler(raw_data_test)\n",
    "\n",
    "data_maxabs_scaler_train = transform_data_maxabs_scaler(data_train_under)\n",
    "data_maxabs_scaler_test = transform_data_maxabs_scaler(raw_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "banner-tension",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, dc_code: string, qty: bigint, brand: int, days_diff: int, label: int]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-symphony",
   "metadata": {},
   "source": [
    "## ML proses Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "german-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model_task_1 = {}\n",
    "metric_task_1 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-proposition",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "romantic-graduation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "crude-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_original = lsvc_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "printable-local",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-5173d71a232f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsvc_tuning_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_original_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_model_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_model_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_model_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdict_model_task_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'svm_original'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsvc_tuning_original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-dda4611a9a37>\u001b[0m in \u001b[0;36mmetric_model_f1\u001b[0;34m(pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmetric_model_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetricName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"fMeasureByLabel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetricLabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred = lsvc_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['svm_original'] = lsvc_tuning_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('svm', 'original', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-treatment",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "collective-cameroon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.cache()\n",
    "data_norm_inf_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "comparable-abraham",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_norm_inf = lsvc_train_tuning_model(data_norm_inf_train,'norm_inf')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "changed-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lsvc_tuning_norm_inf.transform(data_norm_inf_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['svm_norm_inf'] = lsvc_tuning_norm_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "wired-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308782  53081]\n",
      " [  9275   6260]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "wireless-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('svm', 'norm_inf', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "impressive-amsterdam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.unpersist()\n",
    "data_norm_inf_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "mysterious-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.cache()\n",
    "data_norm_1_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "corrected-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_norm_1 = lsvc_train_tuning_model(data_norm_1_train,'norm_1')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "unlimited-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lsvc_tuning_norm_1.transform(data_norm_1_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['svm_norm_1'] = lsvc_tuning_norm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bacterial-sending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[361863      0]\n",
      " [ 15535      0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "worst-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('svm', 'norm_1', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "temporal-valuable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.unpersist()\n",
    "data_norm_1_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "discrete-arrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.cache()\n",
    "data_standard_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "global-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_scaled_features = lsvc_train_tuning_model(data_standard_scaler_train,'scaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "consolidated-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_scaled_features.transform(data_standard_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['svm_scaled_features'] = lsvc_tuning_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "educated-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[306972  54891]\n",
      " [  9107   6428]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "communist-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('svm', 'scaled_features', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "spectacular-madrid",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.unpersist()\n",
    "data_standard_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "sought-weather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.cache()\n",
    "data_minmax_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "about-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_mmscaled_features = lsvc_train_tuning_model(data_minmax_scaler_train,'mmscaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "portuguese-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lsvc_tuning_mmscaled_features.transform(data_minmax_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['svm_mmscaled_features'] = lsvc_tuning_mmscaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "faced-purse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[306973  54890]\n",
      " [  9107   6428]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "western-things",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('svm', 'mmscaled_features', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "advisory-beauty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.unpersist()\n",
    "data_minmax_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "technical-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.cache()\n",
    "data_maxabs_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "female-municipality",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lsvc_tuning_mascaled_features = lsvc_train_tuning_model(data_maxabs_scaler_train,'mascaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "enabling-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lsvc_tuning_mascaled_features.transform(data_maxabs_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['svm_mascaled_features'] = lsvc_tuning_mascaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "impressive-airline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[306972  54891]\n",
      " [  9107   6428]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "opposed-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('svm', 'mascaled_features', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "noble-bunch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.unpersist()\n",
    "data_maxabs_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-proportion",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "governmental-strategy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "empty-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_original = lr_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "comparable-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['lr_original'] = lr_tuning_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "patent-emphasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[233115  20245]\n",
      " [  1378  34997]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.96    253360\n",
      "           1       0.63      0.96      0.76     36375\n",
      "\n",
      "    accuracy                           0.93    289735\n",
      "   macro avg       0.81      0.94      0.86    289735\n",
      "weighted avg       0.95      0.93      0.93    289735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "technical-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('lr', 'original', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "stuck-album",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "minor-photographer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.cache()\n",
    "data_norm_inf_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "productive-syndicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lr_tuning_norm_inf = lr_train_tuning_model(data_norm_inf_train,'norm_inf')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "distant-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lr_tuning_norm_inf.transform(data_norm_inf_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['lr_norm_inf'] = lr_tuning_norm_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "thermal-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284986  76877]\n",
      " [  8235   7300]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "hundred-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('lr', 'norm_inf', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "parental-statistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train.unpersist()\n",
    "data_norm_inf_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "other-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.cache()\n",
    "data_norm_1_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "unavailable-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lr_tuning_norm_1 = lr_train_tuning_model(data_norm_1_train,'norm_1')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "extended-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = lr_tuning_norm_1.transform(data_norm_1_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['lr_norm_1'] = lr_tuning_norm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "spatial-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226827 135036]\n",
      " [  5916   9619]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "organized-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('lr', 'norm_1', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "concrete-monday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train.unpersist()\n",
    "data_norm_1_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "conscious-array",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.cache()\n",
    "data_standard_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "offensive-motivation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "lr_tuning_scaled_features = lr_train_tuning_model(data_standard_scaler_train,'scaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "present-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_scaled_features.transform(data_standard_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['lr_scaled_features'] = lr_tuning_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "fundamental-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[233197 128666]\n",
      " [  6064   9471]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "spiritual-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('lr', 'scaled_features', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "latter-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train.unpersist()\n",
    "data_standard_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "limited-accommodation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.cache()\n",
    "data_minmax_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "frank-moore",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_mmscaled_features = lr_train_tuning_model(data_minmax_scaler_train,'mmscaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "absent-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_mmscaled_features.transform(data_minmax_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['lr_mmscaled_features'] = lr_tuning_mmscaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "rural-housing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[277408  84455]\n",
      " [  7772   7763]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "medieval-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('lr', 'mmscaled_features', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "interior-league",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train.unpersist()\n",
    "data_minmax_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "third-october",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.cache()\n",
    "data_maxabs_scaler_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ahead-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "lr_tuning_mascaled_features = lr_train_tuning_model(data_maxabs_scaler_train,'mascaled_features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "hydraulic-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_tuning_mascaled_features.transform(data_maxabs_scaler_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['lr_mascaled_features'] = lr_tuning_mascaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "current-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[235530 126333]\n",
      " [  6157   9378]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "inappropriate-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('lr', 'mascaled_features', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "solved-colombia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train.unpersist()\n",
    "data_maxabs_scaler_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-platform",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-broad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "rf_tuning_original = rf_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['rf_original'] = rf_tuning_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-organizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('rf', 'original', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-chancellor",
   "metadata": {},
   "source": [
    "## GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-joyce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "gbt_tuning_original = gbt_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gbt_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['gbt_original'] = gbt_tuning_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('gbt', 'original', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-mouth",
   "metadata": {},
   "source": [
    "## DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_train.cache()\n",
    "data_original_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dt_tuning_original = dt_train_tuning_model(data_original_train,'features')\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-roommate",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_tuning_original.transform(data_original_test)\n",
    "f1 = metric_model_f1(pred)\n",
    "precision = metric_model_precision(pred)\n",
    "recall = metric_model_recall(pred)\n",
    "dict_model_task_1['dt_original'] = dt_tuning_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-volleyball",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_task_1.append(('dt', 'original', f1, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_train.unpersist()\n",
    "data_original_test.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-needle",
   "metadata": {},
   "source": [
    "## Metric task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "agricultural-estate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_columns_task_1 = [\"name_model\",\"normalize\",\"f1\",\"precision\",\"recall\"]\n",
    "metric_df_task_1 = spark.createDataFrame(data=metric_task_1, schema = metric_columns_task_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "arabic-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df_task_1 = metric_df_task_1.sort(col('f1').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "special-suite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------+------------------+------------------+\n",
      "|name_model|normalize|                f1|         precision|            recall|\n",
      "+----------+---------+------------------+------------------+------------------+\n",
      "|        lr| original|0.7639848499732582|0.6335215958871873|0.9621168384879725|\n",
      "+----------+---------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_df_task_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-numbers",
   "metadata": {},
   "source": [
    "## Extract best model task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "acting-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_best = metric_df_task_1.select('name_model').collect()[0][0]\n",
    "normalize_best = metric_df_task_1.select('normalize').collect()[0][0]\n",
    "name_best_model = name_best + '_' + normalize_best\n",
    "best_model = dict_model_task_1[name_best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "selected-pizza",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid=LogisticRegression_205e72dc0a21, numClasses=2, numFeatures=9"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "personalized-delta",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1924.save.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"k8\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-068423473426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'k8://saved_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1924.save.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"k8\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3281)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "best_model.write().overwrite().save('k8://saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fewer-century",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1835.load.\n: java.lang.UnsupportedOperationException: empty collection\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1465)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1463)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1298)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-89675ec279d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1835.load.\n: java.lang.UnsupportedOperationException: empty collection\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1465)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1463)\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1298)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "a = LogisticRegressionModel.load(\"saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-elder",
   "metadata": {},
   "source": [
    "## Doing predict Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "afraid-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc_predict():\n",
    "    # Slicing data agar 6 bulan saja\n",
    "    order_bc_tmp = order_bc.select('order_id', 'date_order', 'customer_id','brand')\n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    # Ekstrak fitur quantity, days_diff, dc_code, dan count_order\n",
    "    qty_order = order_product_bc.groupBy('order_id').agg(sum('line_subtotal').alias('revenue'), sum('qty').alias('qty'))\\\n",
    "    .select('order_id', 'revenue','qty')\n",
    "\n",
    "    dc_user = order_shipping_bc.select('order_id', 'shipping_province')\n",
    "    \n",
    "    days_diff = order_bc_extract.select('date_order', 'customer_id')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('date_order')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_order'])\n",
    "    days_diff = days_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"customer_id\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(qty_order, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.join(dc_user, ['order_id'])\n",
    "    order_bc_extract = order_bc_extract.withColumn('count_order', lit(1))\n",
    "    order_bc_extract = order_bc_extract.withColumn('max_extract_month', lit(predict_month))\n",
    "    \n",
    "    final_data = order_bc_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),sum('revenue').alias('revenue'), max('shipping_province').alias('dc_code'),\n",
    "         max('date_order').alias('max_tgl'),max('max_extract_month').alias('max_extract_month'),\n",
    "         size(collect_set('brand')).alias('brand'), sum('qty').alias('qty'))\\\n",
    "    .withColumn('recency', datediff(last_day(col('max_extract_month')), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code','qty','brand')\n",
    "    \n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "numerous-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee_predict():\n",
    "    order_shopee_tmp = order_shopee.select('ordersn', 'create_time', 'buyer_username', 'recipient_province','shopid')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    qty_order = order_product_shopee.groupBy('ordersn').agg(sum('original_price').alias('revenue'), sum('qty_purchased').alias('qty'))\\\n",
    "    .select('ordersn','revenue','qty')\n",
    "    \n",
    "    days_diff = order_shopee_extract.select('create_time', 'buyer_username')\n",
    "    days_diff = days_diff.withColumn('date_only', to_date(col('create_time')))\n",
    "    w = Window().partitionBy('buyer_username').orderBy(['buyer_username', 'create_time'])\n",
    "    days_diff = days_diff.dropDuplicates([\"buyer_username\",\"date_only\"])\n",
    "    days_diff = days_diff.select(\"*\", lag(\"date_only\").over(w).alias(\"new_col_1\"))\\\n",
    "    .withColumn('days_diff', datediff(col('date_only'),col(\"new_col_1\"))).fillna(999,subset=['days_diff'])\n",
    "    days_diff = days_diff.groupby(\"buyer_username\").agg(min('days_diff').alias('days_diff'))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(qty_order, ['ordersn'])\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('count_order', lit(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('max_extract_month', lit(predict_month))\n",
    "    \n",
    "    final_data = order_shopee_extract.groupBy('customer_id')\\\n",
    "    .agg(sum('count_order').alias('frequency'),max('recipient_province').alias('dc_code'),sum('revenue').alias('revenue'),\n",
    "         max('create_time').alias('max_tgl'),max('max_extract_month').alias('max_extract_month'),\n",
    "         size(collect_set('shopid')).alias('brand'),sum('qty').alias('qty'))\\\n",
    "    .withColumn('recency', datediff(last_day(col('max_extract_month')), col('max_tgl')))\\\n",
    "    .select('customer_id','frequency','revenue','recency','dc_code','qty','brand')\n",
    "    \n",
    "    days_diff = days_diff.withColumnRenamed('buyer_username','customer_id')\n",
    "    final_data = final_data.join(days_diff, ['customer_id'])\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "weird-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_feature_predict(data):\n",
    "    indexer = StringIndexer(inputCol='dc_code', outputCol='dc_code_num')\n",
    "    indexd_data=indexer.fit(data).transform(data)\n",
    "    encoder = OneHotEncoder(inputCol='dc_code_num', outputCol = 'dc_code_vec')\n",
    "    onehotdata = encoder.fit(indexd_data).transform(indexd_data)\n",
    "    va = VectorAssembler(outputCol='features')\n",
    "    va.setInputCols(['days_diff','dc_code_vec','qty','brand'])\n",
    "    combine_data = va.transform(onehotdata).select(['customer_id', 'features'])\n",
    "    return combine_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "great-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_predict_data_norm_1(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: 1, normalizer.outputCol:\"norm_1\"})\n",
    "    return data.select('customer_id',\"norm_1\")\n",
    "\n",
    "def transform_predict_data_norm_inf(data):\n",
    "    normalizer = Normalizer(inputCol=\"features\")\n",
    "    data = normalizer.transform(data, {normalizer.p: float(\"inf\"), normalizer.outputCol:\"norm_inf\"})\n",
    "    return data.select('customer_id',\"norm_inf\")\n",
    "\n",
    "def transform_predict_data_standard_scaler(data):\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "    scalerModel = scaler.fit(data)\n",
    "    data = scalerModel.transform(data)\n",
    "    return data.select('customer_id',\"scaled_features\")\n",
    "\n",
    "def transform_predict_data_minmax_scaler(data):\n",
    "    mmscaler = MinMaxScaler(inputCol=\"features\", outputCol=\"mmscaled_features\")\n",
    "    mmscalerModel = mmscaler.fit(data)\n",
    "    data = mmscalerModel.transform(data)\n",
    "    return data.select('customer_id',\"mmscaled_features\")\n",
    "\n",
    "def transform_predict_data_maxabs_scaler(data):\n",
    "    mascaler = MaxAbsScaler(inputCol=\"features\", outputCol=\"mascaled_features\")\n",
    "    mascalerModel = mascaler.fit(data)\n",
    "    data = mascalerModel.transform(data)\n",
    "    return data.select('customer_id',\"mascaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "proof-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bc = transform_data_bc_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "opening-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_shopee = transform_data_shopee_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "social-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_pred = predict_bc.union(predict_shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "secure-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_total_data = combine_feature_predict(final_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bearing-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize_best == 'original':\n",
    "    predict_total_data = predict_total_data\n",
    "elif normalize_best == 'norm_1':\n",
    "    predict_total_data = transform_predict_data_norm_1(predict_total_data)\n",
    "elif normalize_best == 'norm_inf':\n",
    "    predict_total_data = transform_predict_data_norm_inf(predict_total_data)\n",
    "elif normalize_best == 'standard_scaler':\n",
    "    predict_total_data = transform_predict_data_standard_scaler(predict_total_data)\n",
    "elif normalize_best == 'minmax_scaler':\n",
    "    predict_total_data = transform_predict_data_minmax_scaler(predict_total_data)\n",
    "elif normalize_best == 'maxabs_scaler':\n",
    "    predict_total_data = transform_predict_data_maxabs_scaler(predict_total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fitted-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = best_model.transform(predict_total_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "blocked-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_pred_1 = prediction.select('customer_id', 'prediction').withColumnRenamed('prediction','prediksi')\n",
    "hasil_pred_1 = hasil_pred_1.withColumn('potensial_status', when(col('prediksi') == 1, 'potensial').when(col('prediksi') == 0, 'tidak potensial'))\n",
    "hasil_pred_1 = hasil_pred_1.drop('prediksi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-protest",
   "metadata": {},
   "source": [
    "# Extract potential user for task 2 & 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "developing-resolution",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_potential_test = final_data.filter(\"label == 1\").select('customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "advised-engine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_potential_pred = prediction.filter(\"prediction == 1\").select('customer_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-shoulder",
   "metadata": {},
   "source": [
    "# Extract feature for Pattern Mining for Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "confirmed-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_train_model(train_data, item_col):\n",
    "    fpGrowth = FPGrowth(itemsCol=item_col, minSupport=0.001, minConfidence=0.001)\n",
    "    model_fp = fpGrowth.fit(train_data)\n",
    "    return model_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-cleaner",
   "metadata": {},
   "source": [
    "## Extract metric task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "intimate-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bc_task_2():\n",
    "    order_bc_tmp = order_bc.select('order_id', 'date_order', 'customer_id')\n",
    "    \n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    \n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') < predict_month) & \n",
    "                                           (trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    items_purchase = order_product_bc.groupBy('order_id').agg(collect_set('product_name').alias('items'))\\\n",
    "    .select('order_id', 'items')\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(items_purchase, ['order_id'])\n",
    "\n",
    "    final_data = order_bc_extract.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data = final_data.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items_pred'))\\\n",
    "    .select('customer_id','lst_items_pred')\n",
    "    \n",
    "    order_bc_extract_2 = order_bc_tmp.filter((trunc(col('date_order'),'month') == predict_month))\n",
    "    \n",
    "    order_bc_extract_2 = order_bc_extract_2.join(items_purchase, ['order_id'])\n",
    "    \n",
    "    final_data_2 = order_bc_extract_2.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data_2 = final_data_2.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items_true'))\\\n",
    "    .select('customer_id','lst_items_true')\n",
    "    \n",
    "    final_data = final_data.join(final_data_2,['customer_id'],'left')\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "speaking-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shopee_task_2():\n",
    "    order_shopee_tmp = order_shopee.select('ordersn', 'create_time', 'buyer_username')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') < predict_month) & \n",
    "                                                   (trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    items_purchase = order_product_shopee.groupBy('ordersn').agg(collect_set('item_name').alias('items'))\\\n",
    "    .select('ordersn', 'items')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(items_purchase, ['ordersn'])\n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    \n",
    "    final_data = order_shopee_extract.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data = final_data.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items_pred'))\\\n",
    "    .select('customer_id','lst_items_pred')\n",
    "    \n",
    "    order_shopee_extract_2 = order_shopee_tmp.filter((trunc(col('create_time'),'month') == predict_month))\n",
    "    \n",
    "    order_shopee_extract_2 = order_shopee_extract_2.join(items_purchase, ['ordersn'])\n",
    "    order_shopee_extract_2 = order_shopee_extract_2.withColumnRenamed('buyer_username','customer_id')\n",
    "    order_shopee_extract_2 = order_shopee_extract_2.withColumnRenamed('shopid','brand')\n",
    "    \n",
    "    final_data_2 = order_shopee_extract_2.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data_2 = final_data_2.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items_true'))\\\n",
    "    .select('customer_id','lst_items_true')\n",
    "    \n",
    "    final_data = final_data.join(final_data_2,['customer_id'],'left')\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "angry-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bc_task_2 = bc_task_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "identified-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shopee_task_2 = shopee_task_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "interpreted-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total_task_2 = data_bc_task_2.union(data_shopee_task_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "printable-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total_task_2 = data_total_task_2.withColumn(\"lst_items_true\", coalesce('lst_items_true', array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "split-button",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, lst_items_pred: array<string>, lst_items_true: array<string>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_total_task_2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "unique-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp = fp_train_model(data_total_task_2,'lst_items_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "adjacent-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_user_potential_task_2 = data_total_task_2.join(user_potential_test, ['customer_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "caroline-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_task_2 = model_fp.transform(data_user_potential_task_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "physical-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_task_2 = result_task_2.withColumn(\"intersect\", size(array_intersect(\"lst_items_true\", \"prediction\")))\\\n",
    "# .withColumn(\"union\", size(array_union(\"lst_items_true\", \"prediction\"))).withColumn(\"score\", col('intersect')/col('union')).fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "continental-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = result_task_2.agg(avg(col('score'))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "exceptional-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-angel",
   "metadata": {},
   "source": [
    "## Doing predict task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "former-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc_2():\n",
    "    order_bc_tmp = order_bc.select('order_id', 'date_order', 'customer_id')\n",
    "    \n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(user_potential_pred,['customer_id'])\n",
    "    \n",
    "    items_purchase = order_product_bc.groupBy('order_id').agg(collect_set('product_name').alias('items'))\\\n",
    "    .select('order_id', 'items')\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(items_purchase, ['order_id'])\n",
    "\n",
    "    final_data = order_bc_extract.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data = final_data.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items_pred'))\\\n",
    "    .select('customer_id','lst_items_pred')\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "prescribed-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee_2():\n",
    "    order_shopee_tmp = order_shopee.select('ordersn', 'create_time', 'buyer_username')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    order_shopee_extract = order_shopee_extract.join(user_potential_pred,['customer_id'])\n",
    "    \n",
    "    items_purchase = order_product_shopee.groupBy('ordersn').agg(collect_set('item_name').alias('items'))\\\n",
    "    .select('ordersn', 'items')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(items_purchase, ['ordersn'])\n",
    "    \n",
    "    final_data = order_shopee_extract.withColumn(\"explode\", explode(col(\"items\")))\n",
    "    final_data = final_data.groupBy('customer_id')\\\n",
    "    .agg(collect_set('explode').alias('lst_items_pred'))\\\n",
    "    .select('customer_id','lst_items_pred')\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "apart-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_bc = transform_data_bc_2()\n",
    "data_item_shopee = transform_data_shopee_2()\n",
    "data_item_total = data_item_bc.union(data_item_shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "virgin-badge",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, lst_items_pred: array<string>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_item_total.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "coupled-dressing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = model_fp.transform(data_item_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "practical-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_pred_2 = result.select('customer_id','prediction').withColumn(\"item_prediksi\", explode(col(\"prediction\"))).drop('prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-baseball",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-deployment",
   "metadata": {},
   "source": [
    "## Extract feature task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "direct-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee_task_3():\n",
    "    order_shopee_tmp = order_shopee.select('buyer_username','create_time')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    \n",
    "    max_extract_month = predict_month + relativedelta(months=-1)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') < predict_month) & \n",
    "                                                   (trunc(col('create_time'),'month') >= min_month))\n",
    "#     order_shopee_extract_label = order_shopee_tmp.filter((trunc(col('create_time'),'month') == predict_month))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "#     order_shopee_extract_label = order_shopee_extract_label.withColumnRenamed('buyer_username','customer_id')\n",
    "#     order_shopee_extract_label = order_shopee_extract_label.join(user_potential_test,['customer_id'])\n",
    "    order_shopee_extract = order_shopee_extract.join(user_potential_test,['customer_id'])\n",
    "    \n",
    "    mean_day_diff = order_shopee_extract.withColumn('date_only', to_date(col('create_time'))).drop('create_time')\n",
    "    w_mean = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    mean_day_diff = mean_day_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    mean_day_diff = mean_day_diff.select(\"*\", lag(\"date_only\",1).over(w_mean).alias(\"t1_trans_date\"))\n",
    "    mean_day_diff = mean_day_diff.withColumn('t1_day_diff', datediff(col('date_only'),col(\"t1_trans_date\")))\n",
    "    mean_day_diff = mean_day_diff.groupby('customer_id').agg(mean('t1_day_diff').alias('mean_day_diff'))\n",
    "    mean_day_diff = mean_day_diff.fillna(999)\n",
    "    \n",
    "#     max_purchase = order_shopee_extract.groupby('customer_id').agg(to_date(max('create_time')).alias('max_purchase'))\n",
    "#     min_purchase = order_shopee_extract_label.groupby('customer_id').agg(to_date(min('create_time')).alias('min_purchase'))\n",
    "    \n",
    "#     tx_purchase_dates =  max_purchase.join(min_purchase, ['customer_id'], 'left')\\\n",
    "#                                      .withColumn('next_purchase', datediff(col('min_purchase'), col('max_purchase')))\\\n",
    "#                                      .drop('min_purchase', 'max_purchase')\n",
    "    \n",
    "#     tx_purchase_dates = tx_purchase_dates.withColumn('label', lit(2))\n",
    "#     tx_purchase_dates = tx_purchase_dates.withColumn('label', when(col('next_purchase') > 20, 1).otherwise(col('label')))\n",
    "#     tx_purchase_dates = tx_purchase_dates.withColumn('label', when(col('next_purchase') > 50, 0).otherwise(col('label')))\n",
    "    \n",
    "#     max_all_tgl = order_shopee_extract.agg(max('create_time')).collect()[0][0]\n",
    "#     order_shopee_extract = order_shopee_extract.withColumn('max_all_tgl', lit(max_all_tgl))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumn('max_extract_month', lit(max_extract_month))\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('date_only', to_date(col('create_time'))).drop('create_time')\n",
    "    \n",
    "    label = order_shopee_extract.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    label = label.withColumn('days', date_format(col(\"date_only\"), \"d\"))\n",
    "    label = label.withColumn('week', when(col('days')>21,4).when(col('days')>14,3)\\\n",
    "                                                          .when(col('days')>7,2).otherwise(1))\n",
    "    \n",
    "    label = label.groupby('customer_id','week').agg(count('week').alias('count'))\n",
    "    w_label = Window().partitionBy('customer_id').orderBy(col('count').desc(),col('week').desc())\n",
    "    label = label.withColumn('Rank',row_number().over(w_label)).filter(\"Rank == 1\").drop('Rank','count')\n",
    "    label = label.withColumnRenamed('week','label')\n",
    "    \n",
    "#     order_shopee_extract = order_shopee_extract.groupby('customer_id').agg(round(mean(col('week'))).alias('label'))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumn('recency', datediff(last_day(col('max_extract_month')), col('date_only')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    order_shopee_extract = order_shopee_extract.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    order_shopee_extract = order_shopee_extract.select(\"*\", lag(\"date_only\",1).over(w).alias(\"t1_trans_date\"))\n",
    "    order_shopee_extract = order_shopee_extract.select(\"*\", lag(\"date_only\",2).over(w).alias(\"t2_trans_date\"))\n",
    "    order_shopee_extract = order_shopee_extract.select(\"*\", lag(\"date_only\",3).over(w).alias(\"t3_trans_date\"))\n",
    "    \n",
    "#     order_shopee_extract = order_shopee_extract.withColumn('t1_day_diff', datediff(col('date_only'),col(\"t1_trans_date\")))\\\n",
    "#                                                .withColumn('t2_day_diff', datediff(col('date_only'),col(\"t2_trans_date\")))\\\n",
    "#                                                .withColumn('t3_day_diff', datediff(col('date_only'),col(\"t3_trans_date\")))\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('t1_day', date_format(col(\"date_only\"), \"d\").cast(IntegerType()))\\\n",
    "                                       .withColumn('t2_day', date_format(col(\"t1_trans_date\"), \"d\").cast(IntegerType()))\\\n",
    "                                       .withColumn('t3_day', date_format(col(\"t3_trans_date\"), \"d\").cast(IntegerType()))\n",
    "    \n",
    "#     order_shopee_extract = order_shopee_extract.withColumn('last_1_day', date_format(col(\"date_only\"), \"d\").cast(IntegerType()))\n",
    "#     order_shopee_extract = order_shopee_extract.withColumn('last_1_week', when(col('last_1_day')>21,4).when(col('last_1_day')>14,3)\\\n",
    "#                                                           .when(col('last_1_day')>7,2).otherwise(1))\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.fillna(999)\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('last_1_week', when(col('t1_day')>21,4).when(col('t1_day')>14,3)\\\n",
    "                                                          .when(col('t1_day')>7,2).otherwise(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('last_2_week', when(col('t2_day')>21,4).when(col('t2_day')>14,3)\\\n",
    "                                                          .when(col('t2_day')>7,2).otherwise(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('last_3_week', when(col('t3_day')>21,4).when(col('t3_day')>14,3)\\\n",
    "                                                          .when(col('t3_day')>7,2).otherwise(1))\n",
    "    \n",
    "#     date_diff_std_mean = order_shopee_extract.groupby('customer_id').agg(mean('t1_day_diff').alias('mean_day_diff'),\n",
    "#                                                                          stddev('t1_day_diff').alias('std_day_diff'))\n",
    "    \n",
    "    w_2 = Window().partitionBy('customer_id').orderBy(col('date_only').desc())\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('Rank',row_number().over(w_2)).filter(\"Rank == 1\").drop('Rank')\n",
    "    \n",
    "#     order_shopee_extract = order_shopee_extract.join(date_diff_std_mean, ['customer_id'])\n",
    "#     order_shopee_extract = order_shopee_extract.fillna(0,subset=['std_day_diff'])\n",
    "#     order_shopee_extract = order_shopee_extract.join(tx_purchase_dates, ['customer_id'])\n",
    "    order_shopee_extract = order_shopee_extract.join(mean_day_diff, ['customer_id'])\n",
    "    order_shopee_extract = order_shopee_extract.join(label, ['customer_id'])\n",
    "    order_shopee_extract = order_shopee_extract.drop('date_only', 't1_trans_date', 't2_trans_date', 't3_trans_date','max_extract_month','days','week')\n",
    "    \n",
    "    return order_shopee_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "attractive-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc_task_3():\n",
    "    order_bc_tmp = order_bc.select('date_order', 'customer_id')\n",
    "    \n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-6)\n",
    "    \n",
    "    max_extract_month = predict_month + relativedelta(months=-1)\n",
    "    \n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') < predict_month) & \n",
    "                                           (trunc(col('date_order'),'month') >= min_month))\n",
    "#     order_bc_extract_label = order_bc_tmp.filter((trunc(col('date_order'),'month') == predict_month))\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(user_potential_test,['customer_id'])\n",
    "    \n",
    "    mean_day_diff = order_bc_extract.withColumn('date_only', to_date(col('date_order'))).drop('date_order')\n",
    "    w_mean = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    mean_day_diff = mean_day_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    mean_day_diff = mean_day_diff.select(\"*\", lag(\"date_only\",1).over(w_mean).alias(\"t1_trans_date\"))\n",
    "    mean_day_diff = mean_day_diff.withColumn('t1_day_diff', datediff(col('date_only'),col(\"t1_trans_date\")))\n",
    "    mean_day_diff = mean_day_diff.groupby('customer_id').agg(mean('t1_day_diff').alias('mean_day_diff'))\n",
    "    mean_day_diff = mean_day_diff.fillna(999)\n",
    "    \n",
    "#     order_bc_extract_label = order_bc_extract_label.join(user_potential_test,['customer_id'])\n",
    "    \n",
    "#     max_purchase = order_bc_extract.groupby('customer_id').agg(to_date(max('date_order')).alias('max_purchase'))\n",
    "#     min_purchase = order_bc_extract_label.groupby('customer_id').agg(to_date(min('date_order')).alias('min_purchase'))\n",
    "    \n",
    "#     tx_purchase_dates =  max_purchase.join(min_purchase, ['customer_id'], 'left')\\\n",
    "#                                      .withColumn('next_purchase', datediff(col('min_purchase'), col('max_purchase')))\\\n",
    "#                                      .drop('min_purchase', 'max_purchase')\n",
    "    \n",
    "#     tx_purchase_dates = tx_purchase_dates.withColumn('label', lit(2))\n",
    "#     tx_purchase_dates = tx_purchase_dates.withColumn('label', when(col('next_purchase') > 20, 1).otherwise(col('label')))\n",
    "#     tx_purchase_dates = tx_purchase_dates.withColumn('label', when(col('next_purchase') > 50, 0).otherwise(col('label')))\n",
    "    \n",
    "#     max_all_tgl = order_bc_extract.agg(max('date_order')).collect()[0][0]\n",
    "#     order_bc_extract = order_bc_extract.withColumn('max_all_tgl', lit(max_all_tgl))\n",
    "                                                      \n",
    "    order_bc_extract = order_bc_extract.withColumn('max_extract_month', lit(max_extract_month))\n",
    "    order_bc_extract = order_bc_extract.withColumn('date_only', to_date(col('date_order'))).drop('date_order')\n",
    "    \n",
    "    label = order_bc_extract.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    label = label.withColumn('days', date_format(col(\"date_only\"), \"d\"))\n",
    "    label = label.withColumn('week', when(col('days')>21,4).when(col('days')>14,3)\\\n",
    "                                                          .when(col('days')>7,2).otherwise(1))\n",
    "    label = label.groupby('customer_id','week').agg(count('week').alias('count'))\n",
    "    w_label = Window().partitionBy('customer_id').orderBy(col('count').desc(),col('week').desc())\n",
    "    label = label.withColumn('Rank',row_number().over(w_label)).filter(\"Rank == 1\").drop('Rank','count')\n",
    "    \n",
    "    label = label.withColumnRenamed('week','label')\n",
    "    \n",
    "#     order_bc_extract = order_bc_extract.groupby('customer_id').agg(round(mean(col('week'))).alias('label'))\n",
    "                                                      \n",
    "    order_bc_extract = order_bc_extract.withColumn('recency', datediff(last_day(col('max_extract_month')), col('date_only')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    order_bc_extract = order_bc_extract.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    order_bc_extract = order_bc_extract.select(\"*\", lag(\"date_only\",1).over(w).alias(\"t1_trans_date\"))\n",
    "    order_bc_extract = order_bc_extract.select(\"*\", lag(\"date_only\",2).over(w).alias(\"t2_trans_date\"))\n",
    "    order_bc_extract = order_bc_extract.select(\"*\", lag(\"date_only\",3).over(w).alias(\"t3_trans_date\"))\n",
    "    \n",
    "#     order_bc_extract = order_bc_extract.withColumn('t1_day_diff', datediff(col('date_only'),col(\"t1_trans_date\")))\\\n",
    "#                                        .withColumn('t2_day_diff', datediff(col('date_only'),col(\"t2_trans_date\")))\\\n",
    "#                                        .withColumn('t3_day_diff', datediff(col('date_only'),col(\"t3_trans_date\")))\n",
    "\n",
    "    order_bc_extract = order_bc_extract.withColumn('t1_day', date_format(col(\"date_only\"), \"d\").cast(IntegerType()))\\\n",
    "                                       .withColumn('t2_day', date_format(col(\"t1_trans_date\"), \"d\").cast(IntegerType()))\\\n",
    "                                       .withColumn('t3_day', date_format(col(\"t3_trans_date\"), \"d\").cast(IntegerType()))\n",
    "    \n",
    "#     order_bc_extract = order_bc_extract.withColumn('last_1_day', date_format(col(\"date_only\"), \"d\").cast(IntegerType()))\n",
    "\n",
    "    order_bc_extract = order_bc_extract.fillna(999)\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.withColumn('last_1_week', when(col('t1_day')>21,4).when(col('t1_day')>14,3)\\\n",
    "                                                          .when(col('t1_day')>7,2).otherwise(1))\n",
    "    order_bc_extract = order_bc_extract.withColumn('last_2_week', when(col('t2_day')>21,4).when(col('t2_day')>14,3)\\\n",
    "                                                          .when(col('t2_day')>7,2).otherwise(1))\n",
    "    order_bc_extract = order_bc_extract.withColumn('last_3_week', when(col('t3_day')>21,4).when(col('t3_day')>14,3)\\\n",
    "                                                          .when(col('t3_day')>7,2).otherwise(1))\n",
    "    \n",
    "#     date_diff_std_mean = order_bc_extract.groupby('customer_id').agg(mean('t1_day_diff').alias('mean_day_diff'),\n",
    "#                                                                      stddev('t1_day_diff').alias('std_day_diff'))\n",
    "    \n",
    "    w_2 = Window().partitionBy('customer_id').orderBy(col('date_only').desc())\n",
    "    order_bc_extract = order_bc_extract.withColumn('Rank',row_number().over(w_2)).filter(\"Rank == 1\").drop('Rank')\n",
    "    \n",
    "#     order_bc_extract = order_bc_extract.join(date_diff_std_mean, ['customer_id'])\n",
    "#     order_bc_extract = order_bc_extract.fillna(0,subset=['std_day_diff'])\n",
    "#     order_bc_extract = order_bc_extract.join(tx_purchase_dates, ['customer_id'])\n",
    "    order_bc_extract = order_bc_extract.join(mean_day_diff, ['customer_id'])\n",
    "    order_bc_extract = order_bc_extract.join(label, ['customer_id'])\n",
    "    order_bc_extract = order_bc_extract.drop('date_only', 't1_trans_date', 't2_trans_date', 't3_trans_date','max_extract_month','days','week')\n",
    "    \n",
    "    return order_bc_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "catholic-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shopee_task_3 = transform_data_shopee_task_3()\n",
    "data_bc_task_3 = transform_data_bc_task_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "talented-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_task_3 = data_shopee_task_3.union(data_bc_task_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-auckland",
   "metadata": {},
   "source": [
    "## prepare data for predict task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "boring-belgium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[customer_id: string, recency: int, t1_day: int, t2_day: int, t3_day: int, last_1_week: int, last_2_week: int, last_3_week: int, mean_day_diff: double, label: int]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final_task_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "necessary-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol='features')\n",
    "va.setInputCols(['recency','t1_day','t2_day','mean_day_diff','last_1_week','last_2_week'])\n",
    "data_final_task_3 = va.transform(data_final_task_3).select(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "sacred-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task_3, test_task_3 = data_final_task_3.randomSplit([0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "alien-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_train_3 = train_task_3\n",
    "data_original_test_3 = test_task_3\n",
    "\n",
    "data_norm_1_train_3 = transform_data_norm_1(train_task_3)\n",
    "data_norm_1_test_3 = transform_data_norm_1(test_task_3)\n",
    "\n",
    "data_norm_inf_train_3 = transform_data_norm_inf(train_task_3)\n",
    "data_norm_inf_test_3 = transform_data_norm_inf(test_task_3)\n",
    "\n",
    "data_standard_scaler_train_3 = transform_data_standard_scaler(train_task_3)\n",
    "data_standard_scaler_test_3 = transform_data_standard_scaler(test_task_3)\n",
    "\n",
    "data_minmax_scaler_train_3 = transform_data_minmax_scaler(train_task_3)\n",
    "data_minmax_scaler_test_3 = transform_data_minmax_scaler(test_task_3)\n",
    "\n",
    "data_maxabs_scaler_train_3 = transform_data_maxabs_scaler(train_task_3)\n",
    "data_maxabs_scaler_test_3 = transform_data_maxabs_scaler(test_task_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "announced-improvement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final_task_3.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-feedback",
   "metadata": {},
   "source": [
    "## predict task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "increasing-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_model_task_3 = {}\n",
    "metric_task_3 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-sweden",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "discrete-clothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train_3.cache()\n",
    "data_original_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "patent-indicator",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_task_3 = lr_train_tuning_model(data_original_train_3,'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "greater-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_task_3.transform(data_original_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['lr_original'] = lr_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "irish-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  463  1429     0   388]\n",
      " [  168 10620     0  1996]\n",
      " [  173  2324     0  2282]\n",
      " [   23  1670     0 14546]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.20      0.30      2280\n",
      "           2       0.66      0.83      0.74     12784\n",
      "           3       0.00      0.00      0.00      4779\n",
      "           4       0.76      0.90      0.82     16239\n",
      "\n",
      "    accuracy                           0.71     36082\n",
      "   macro avg       0.49      0.48      0.46     36082\n",
      "weighted avg       0.61      0.71      0.65     36082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('lr', 'original', f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "junior-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train_3.unpersist()\n",
    "data_original_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "liberal-screening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train_3.cache()\n",
    "data_norm_inf_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "affected-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_task_3 = lr_train_tuning_model(data_norm_inf_train_3,'norm_inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "micro-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_task_3.transform(data_norm_inf_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['lr_norm_inf'] = lr_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "departmental-rotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  1680     0   600]\n",
      " [    0  7876     0  4908]\n",
      " [    0  2685     0  2094]\n",
      " [    0  4504     0 11735]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00      2280\n",
      "           2       0.47      0.62      0.53     12784\n",
      "           3       0.00      0.00      0.00      4779\n",
      "           4       0.61      0.72      0.66     16239\n",
      "\n",
      "    accuracy                           0.54     36082\n",
      "   macro avg       0.27      0.33      0.30     36082\n",
      "weighted avg       0.44      0.54      0.49     36082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('lr', 'norm_inf', f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "technical-russia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_inf: vector]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_inf_train_3.unpersist()\n",
    "data_norm_inf_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "broke-macro",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train_3.cache()\n",
    "data_norm_1_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "earlier-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_task_3 = lr_train_tuning_model(data_norm_1_train_3,'norm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "modular-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_task_3.transform(data_norm_1_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['lr_norm_1'] = lr_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "theoretical-billy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   58  1741     0   481]\n",
      " [    2  8477     0  4305]\n",
      " [    0  2754     0  2025]\n",
      " [    3  4634     0 11602]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.03      0.05      2280\n",
      "           2       0.48      0.66      0.56     12784\n",
      "           3       0.00      0.00      0.00      4779\n",
      "           4       0.63      0.71      0.67     16239\n",
      "\n",
      "    accuracy                           0.56     36082\n",
      "   macro avg       0.51      0.35      0.32     36082\n",
      "weighted avg       0.51      0.56      0.50     36082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('lr', 'norm_1', f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "finished-gnome",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, norm_1: vector]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_norm_1_train_3.unpersist()\n",
    "data_norm_1_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "specific-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train_3.cache()\n",
    "data_standard_scaler_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "generic-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_task_3 = lr_train_tuning_model(data_standard_scaler_train_3,'scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "political-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_task_3.transform(data_standard_scaler_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['lr_scaled_features'] = lr_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "smoking-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  463  1429     0   388]\n",
      " [  166 10626     0  1992]\n",
      " [  173  2334     0  2272]\n",
      " [   23  1671     0 14545]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.20      0.30      2280\n",
      "           2       0.66      0.83      0.74     12784\n",
      "           3       0.00      0.00      0.00      4779\n",
      "           4       0.76      0.90      0.82     16239\n",
      "\n",
      "    accuracy                           0.71     36082\n",
      "   macro avg       0.50      0.48      0.46     36082\n",
      "weighted avg       0.61      0.71      0.65     36082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('lr','scaled_features',f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "desirable-agency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, scaled_features: vector]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standard_scaler_train_3.unpersist()\n",
    "data_standard_scaler_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "right-supply",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train_3.cache()\n",
    "data_minmax_scaler_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "private-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_task_3 = lr_train_tuning_model(data_minmax_scaler_train_3,'mmscaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "pointed-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_task_3.transform(data_minmax_scaler_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['lr_mmscaled_features'] = lr_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "adopted-transparency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  419  1460     0   401]\n",
      " [   73 10663     0  2048]\n",
      " [   43  2155     0  2581]\n",
      " [    7  1553     0 14679]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.18      0.30      2280\n",
      "           2       0.67      0.83      0.75     12784\n",
      "           3       0.00      0.00      0.00      4779\n",
      "           4       0.74      0.90      0.82     16239\n",
      "\n",
      "    accuracy                           0.71     36082\n",
      "   macro avg       0.55      0.48      0.46     36082\n",
      "weighted avg       0.62      0.71      0.65     36082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('lr', 'mmscaled_features', f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "elegant-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mmscaled_features: vector]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minmax_scaler_train_3.unpersist()\n",
    "data_minmax_scaler_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "visible-differential",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train_3.cache()\n",
    "data_maxabs_scaler_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "republican-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_task_3 = lr_train_tuning_model(data_maxabs_scaler_train_3,'mascaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "adjusted-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_task_3.transform(data_maxabs_scaler_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['lr_mascaled_features'] = lr_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "completed-plenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  463  1429     0   388]\n",
      " [  168 10620     0  1996]\n",
      " [  173  2324     0  2282]\n",
      " [   23  1670     0 14546]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.20      0.30      2280\n",
      "           2       0.66      0.83      0.74     12784\n",
      "           3       0.00      0.00      0.00      4779\n",
      "           4       0.76      0.90      0.82     16239\n",
      "\n",
      "    accuracy                           0.71     36082\n",
      "   macro avg       0.49      0.48      0.46     36082\n",
      "weighted avg       0.61      0.71      0.65     36082\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('lr','mascaled_features', f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "piano-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: int, mascaled_features: vector]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_maxabs_scaler_train_3.unpersist()\n",
    "data_maxabs_scaler_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-experience",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "natural-bonus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train_3.cache()\n",
    "data_original_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "loving-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_task_3 = dt_train_tuning_model(data_original_train_3,'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "saving-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dt_task_3.transform(data_original_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['dt_original'] = dt_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "unsigned-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1319   276   168   517]\n",
      " [  977  9604   586  1617]\n",
      " [   47   114  4355   263]\n",
      " [  106   238   380 15515]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.58      0.56      2280\n",
      "           2       0.94      0.75      0.83     12784\n",
      "           3       0.79      0.91      0.85      4779\n",
      "           4       0.87      0.96      0.91     16239\n",
      "\n",
      "    accuracy                           0.85     36082\n",
      "   macro avg       0.78      0.80      0.79     36082\n",
      "weighted avg       0.86      0.85      0.85     36082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('dt', 'original', f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "automatic-niagara",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train_3.unpersist()\n",
    "data_original_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-joseph",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "signal-float",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train_3.cache()\n",
    "data_original_test_3.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "potential-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_task_3 = rf_train_tuning_model(data_original_train_3,'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "persistent-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = rf_task_3.transform(data_original_test_3)\n",
    "accuracy = metric_model_accuracy(pred)\n",
    "dict_model_task_3['rf_original'] = rf_task_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "outstanding-monkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1148   445   168   518]\n",
      " [   17 10575   586  1617]\n",
      " [    7   154  4361   262]\n",
      " [   16   328   380 15509]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.50      0.66      2279\n",
      "           2       0.92      0.83      0.87     12795\n",
      "           3       0.79      0.91      0.85      4784\n",
      "           4       0.87      0.96      0.91     16233\n",
      "\n",
      "    accuracy                           0.88     36091\n",
      "   macro avg       0.89      0.80      0.82     36091\n",
      "weighted avg       0.88      0.88      0.87     36091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = pred.select(['label']).collect()\n",
    "y_pred = pred.select(['prediction']).collect()\n",
    "f1 = metric_model_f1_multi(y_true,y_pred)\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "print(classification_report(y_true,y_pred))\n",
    "metric_task_3.append(('rf', 'original', f1.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "technical-publication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_original_train_3.unpersist()\n",
    "data_original_test_3.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-genome",
   "metadata": {},
   "source": [
    "## Export metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "brown-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_columns_task_3 = [\"name_model\",\"normalize\",\"f1\",\"accuracy\"]\n",
    "metric_df_task_3 = spark.createDataFrame(data=metric_task_3, schema = metric_columns_task_3)\n",
    "metric_df_task_3 = metric_df_task_3.sort(['f1','accuracy'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "intimate-crossing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------+-----------------+\n",
      "|name_model|normalize|                f1|         accuracy|\n",
      "+----------+---------+------------------+-----------------+\n",
      "|        rf| original|0.8224569008303385|0.875370591006068|\n",
      "+----------+---------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_df_task_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "classified-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_best_3 = metric_df_task_3.select('name_model').collect()[0][0]\n",
    "normalize_best_3 = metric_df_task_3.select('normalize').collect()[0][0]\n",
    "name_best_model_3 = name_best_3 + '_' + normalize_best_3\n",
    "best_model_3 = dict_model_task_3[name_best_model_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "royal-horror",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel: uid=RandomForestClassifier_1ddb3a118396, numTrees=20, numClasses=5, numFeatures=6"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_3.write().overwrite().save(\"/home/notebook/model_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-kenya",
   "metadata": {},
   "source": [
    "## Predict Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "western-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_shopee_task_3_pred():\n",
    "    order_shopee_tmp = order_shopee.select('buyer_username','create_time')\n",
    "    \n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') >= min_month))\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(user_potential_pred,['customer_id'])\n",
    "    \n",
    "    mean_day_diff = order_shopee_extract.withColumn('date_only', to_date(col('create_time'))).drop('create_time')\n",
    "    w_mean = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    mean_day_diff = mean_day_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    mean_day_diff = mean_day_diff.select(\"*\", lag(\"date_only\",1).over(w_mean).alias(\"t1_trans_date\"))\n",
    "    mean_day_diff = mean_day_diff.withColumn('t1_day_diff', datediff(col('date_only'),col(\"t1_trans_date\")))\n",
    "    mean_day_diff = mean_day_diff.groupby('customer_id').agg(mean('t1_day_diff').alias('mean_day_diff'))\n",
    "    mean_day_diff = mean_day_diff.fillna(999)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumn('max_extract_month', lit(predict_month))\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('date_only', to_date(col('create_time'))).drop('create_time')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumn('recency', datediff(last_day(col('max_extract_month')), col('date_only')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    order_shopee_extract = order_shopee_extract.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    order_shopee_extract = order_shopee_extract.select(\"*\", lag(\"date_only\",1).over(w).alias(\"t1_trans_date\"))\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('t1_day', date_format(col(\"date_only\"), \"d\").cast(IntegerType()))\\\n",
    "                                       .withColumn('t2_day', date_format(col(\"t1_trans_date\"), \"d\").cast(IntegerType()))\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.fillna(999)\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('last_1_week', when(col('t1_day')>21,4).when(col('t1_day')>14,3)\\\n",
    "                                                          .when(col('t1_day')>7,2).otherwise(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('last_2_week', when(col('t2_day')>21,4).when(col('t2_day')>14,3)\\\n",
    "                                                          .when(col('t2_day')>7,2).otherwise(1))\n",
    "    \n",
    "    w_2 = Window().partitionBy('customer_id').orderBy(col('date_only').desc())\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('Rank',row_number().over(w_2)).filter(\"Rank == 1\").drop('Rank')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(mean_day_diff, ['customer_id'])\n",
    "    order_shopee_extract = order_shopee_extract.drop('date_only','t1_trans_date','max_extract_month','days','week')\n",
    "    \n",
    "    return order_shopee_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "norwegian-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bc_3():\n",
    "    order_shopee_tmp = order_shopee.select('buyer_username','create_time')\n",
    "    predict_month = order_shopee_tmp.agg(trunc(max(col('create_time')),'month')).collect()[0][0]\n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    order_shopee_extract = order_shopee_tmp.filter((trunc(col('create_time'),'month') >= min_month))\n",
    "    order_shopee_extract = order_shopee_extract.withColumnRenamed('buyer_username','customer_id')\n",
    "    order_shopee_extract = order_shopee_extract.join(user_potential_pred,['customer_id'])\n",
    "    \n",
    "    mean_day_diff = order_shopee_extract.withColumn('date_only', to_date(col('create_time'))).drop('create_time')\n",
    "    w_mean = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    mean_day_diff = mean_day_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    mean_day_diff = mean_day_diff.select(\"*\", lag(\"date_only\",1).over(w_mean).alias(\"t1_trans_date\"))\n",
    "    mean_day_diff = mean_day_diff.withColumn('t1_day_diff', datediff(col('date_only'),col(\"t1_trans_date\")))\n",
    "    mean_day_diff = mean_day_diff.groupby('customer_id').agg(mean('t1_day_diff').alias('mean_day_diff'))\n",
    "    mean_day_diff = mean_day_diff.fillna(999)\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumn('max_extract_month', lit(predict_month))\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('date_only', to_date(col('create_time'))).drop('create_time')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.withColumn('recency', datediff(last_day(col('max_extract_month')), col('date_only')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    order_shopee_extract = order_shopee_extract.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    order_shopee_extract = order_shopee_extract.select(\"*\", lag(\"date_only\",1).over(w).alias(\"t1_trans_date\"))\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('t1_day', date_format(col(\"date_only\"), \"d\").cast(IntegerType()))\\\n",
    "                                       .withColumn('t2_day', date_format(col(\"t1_trans_date\"), \"d\").cast(IntegerType()))\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.fillna(999)\n",
    "\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('last_1_week', when(col('t1_day')>21,4).when(col('t1_day')>14,3)\\\n",
    "                                                          .when(col('t1_day')>7,2).otherwise(1))\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('last_2_week', when(col('t2_day')>21,4).when(col('t2_day')>14,3)\\\n",
    "                                                          .when(col('t2_day')>7,2).otherwise(1))\n",
    "    \n",
    "    w_2 = Window().partitionBy('customer_id').orderBy(col('date_only').desc())\n",
    "    order_shopee_extract = order_shopee_extract.withColumn('Rank',row_number().over(w_2)).filter(\"Rank == 1\").drop('Rank')\n",
    "    \n",
    "    order_shopee_extract = order_shopee_extract.join(mean_day_diff, ['customer_id'])\n",
    "    order_shopee_extract = order_shopee_extract.drop('date_only','t1_trans_date','max_extract_month','days','week')\n",
    "    \n",
    "    return end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "resident-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.test_bc_3()>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bc_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fatty-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_bc_task_3_pred():\n",
    "    order_bc_tmp = order_bc.select('date_order', 'customer_id')\n",
    "    \n",
    "    predict_month = order_bc_tmp.agg(trunc(max(col('date_order')),'month')).collect()[0][0]\n",
    "    \n",
    "    min_month = predict_month + relativedelta(months=-5)\n",
    "    \n",
    "    order_bc_extract = order_bc_tmp.filter((trunc(col('date_order'),'month') >= min_month))\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(user_potential_pred,['customer_id'])\n",
    "    \n",
    "    mean_day_diff = order_bc_extract.withColumn('date_only', to_date(col('date_order'))).drop('date_order')\n",
    "    w_mean = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    mean_day_diff = mean_day_diff.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    mean_day_diff = mean_day_diff.select(\"*\", lag(\"date_only\",1).over(w_mean).alias(\"t1_trans_date\"))\n",
    "    mean_day_diff = mean_day_diff.withColumn('t1_day_diff', datediff(col('date_only'),col(\"t1_trans_date\")))\n",
    "    mean_day_diff = mean_day_diff.groupby('customer_id').agg(mean('t1_day_diff').alias('mean_day_diff'))\n",
    "    mean_day_diff = mean_day_diff.fillna(999)\n",
    "                                                      \n",
    "    order_bc_extract = order_bc_extract.withColumn('max_extract_month', lit(predict_month))\n",
    "    order_bc_extract = order_bc_extract.withColumn('date_only', to_date(col('date_order'))).drop('date_order')\n",
    "                                                      \n",
    "    order_bc_extract = order_bc_extract.withColumn('recency', datediff(last_day(col('max_extract_month')), col('date_only')))\n",
    "    w = Window().partitionBy('customer_id').orderBy(['customer_id', 'date_only'])\n",
    "    order_bc_extract = order_bc_extract.dropDuplicates([\"customer_id\",\"date_only\"])\n",
    "    order_bc_extract = order_bc_extract.select(\"*\", lag(\"date_only\",1).over(w).alias(\"t1_trans_date\"))\n",
    "\n",
    "    order_bc_extract = order_bc_extract.withColumn('t1_day', date_format(col(\"date_only\"), \"d\").cast(IntegerType()))\\\n",
    "                                       .withColumn('t2_day', date_format(col(\"t1_trans_date\"), \"d\").cast(IntegerType()))\n",
    "\n",
    "    order_bc_extract = order_bc_extract.fillna(999)\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.withColumn('last_1_week', when(col('t1_day')>21,4).when(col('t1_day')>14,3)\\\n",
    "                                                          .when(col('t1_day')>7,2).otherwise(1))\n",
    "    order_bc_extract = order_bc_extract.withColumn('last_2_week', when(col('t2_day')>21,4).when(col('t2_day')>14,3)\\\n",
    "                                                          .when(col('t2_day')>7,2).otherwise(1))\n",
    "    \n",
    "    w_2 = Window().partitionBy('customer_id').orderBy(col('date_only').desc())\n",
    "    order_bc_extract = order_bc_extract.withColumn('Rank',row_number().over(w_2)).filter(\"Rank == 1\").drop('Rank')\n",
    "    \n",
    "    order_bc_extract = order_bc_extract.join(mean_day_diff, ['customer_id'])\n",
    "    order_bc_extract = order_bc_extract.drop('date_only','t1_trans_date','max_extract_month','days','week')\n",
    "    \n",
    "    return order_bc_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "lined-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shopee_task_3_pred = transform_data_shopee_task_3_pred()\n",
    "data_bc_task_3_pred = transform_data_bc_task_3_pred()\n",
    "data_final_task_3_pred = data_shopee_task_3_pred.union(data_bc_task_3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "upset-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol='features')\n",
    "va.setInputCols(['recency','t1_day','t2_day','mean_day_diff','last_1_week','last_2_week'])\n",
    "data_final_task_3_pred = va.transform(data_final_task_3_pred).select(['customer_id','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "documented-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "if normalize_best_3 == 'original':\n",
    "    predict_total_data_3 = data_final_task_3_pred\n",
    "elif normalize_best_3 == 'norm_1':\n",
    "    predict_total_data_3 = transform_predict_data_norm_1(data_final_task_3_pred)\n",
    "elif normalize_best_3 == 'norm_inf':\n",
    "    predict_total_data_3 = transform_predict_data_norm_inf(data_final_task_3_pred)\n",
    "elif normalize_best_3 == 'standard_scaler':\n",
    "    predict_total_data_3 = transform_predict_data_standard_scaler(data_final_task_3_pred)\n",
    "elif normalize_best_3 == 'minmax_scaler':\n",
    "    predict_total_data_3 = transform_predict_data_minmax_scaler(data_final_task_3_pred)\n",
    "elif normalize_best_3 == 'maxabs_scaler':\n",
    "    predict_total_data_3 = transform_predict_data_maxabs_scaler(data_final_task_3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "nominated-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_3 = best_model_3.transform(predict_total_data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "conceptual-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil_pred_3 = prediction_3.select('customer_id','prediction').withColumnRenamed('prediction','next_time_buy_code')\n",
    "hasil_pred_3 = hasil_pred_3.withColumn('next_time_buy', when(col('next_time_buy_code') == 1, 'Minggu pertama (tanggal 1-17)')\\\n",
    "                                      .when(col('next_time_buy_code') == 2, 'Minggu kedua (tanggal 8-14)')\\\n",
    "                                      .when(col('next_time_buy_code') == 3, 'Minggu ketiga (tanggal 15-21)')\\\n",
    "                                      .when(col('next_time_buy_code') == 4, 'Minggu keempat (tanggal 22 keatas)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-manitoba",
   "metadata": {},
   "source": [
    "# Adding all result to database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-discipline",
   "metadata": {},
   "source": [
    "## Extract Province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "paperback-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_province_shopee = order_shopee_2.select('buyer_username','recipient_province').groupby('buyer_username')\\\n",
    ".agg(max('recipient_province').alias('provinsi')).withColumnRenamed('buyer_username','customer_id')\n",
    "\n",
    "data_province_bc = order_bc.join(order_shipping_bc_2,['order_id']).select('customer_id','shipping_province')\\\n",
    ".groupby('customer_id').agg(max('shipping_province').alias('provinsi'))\n",
    "\n",
    "data_province = data_province_shopee.union(data_province_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-archives",
   "metadata": {},
   "source": [
    "## Extract Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "digital-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_username_shopee = order_shopee_2.select('buyer_username','recipient_name')\\\n",
    ".withColumnRenamed('buyer_username','customer_id').withColumnRenamed('recipient_name','name')\n",
    "\n",
    "data_username_bc = user_bc.select('user_id','nickname').withColumnRenamed('user_id','customer_id')\\\n",
    ".withColumnRenamed('nickname','name')\n",
    "\n",
    "data_username = data_username_shopee.union(data_username_bc)\n",
    "data_username = data_username.groupby('customer_id').agg(max('name').alias('nama'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-environment",
   "metadata": {},
   "source": [
    "## Extract revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ceramic-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_revenue = final_data_pred.select('customer_id','revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-engineering",
   "metadata": {},
   "source": [
    "## Combine and insert task 1 to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "digital-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_task_1 = hasil_pred_1.join(data_revenue,['customer_id']).join(data_province,['customer_id'],'left')\\\n",
    ".join(data_username,['customer_id'],'left').select('customer_id','nama','revenue','provinsi','potensial_status')\n",
    "\n",
    "combined_task_1 = combined_task_1.na.drop(subset=['nama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "funky-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_task_1.write.format(\"jdbc\")\\\n",
    ".option(\"url\",'jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres')\\\n",
    ".option(\"dbtable\",'public.hasil_task_1')\\\n",
    ".option(\"user\",'postgres')\\\n",
    ".option(\"password\",'postgres')\\\n",
    ".option(\"driver\",\"org.postgresql.Driver\")\\\n",
    ".option(\"truncate\",\"true\")\\\n",
    ".mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-nerve",
   "metadata": {},
   "source": [
    "## Combine and insert task 2 to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "grand-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_task_2 = hasil_pred_2.join(data_username,['customer_id'],'left').select('customer_id','nama','item_prediksi')\n",
    "combined_task_2 = combined_task_2.na.drop(subset=['nama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "proved-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_task_2.write.format(\"jdbc\")\\\n",
    ".option(\"url\",'jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres')\\\n",
    ".option(\"dbtable\",'public.hasil_task_2')\\\n",
    ".option(\"user\",'postgres')\\\n",
    ".option(\"password\",'postgres')\\\n",
    ".option(\"driver\",\"org.postgresql.Driver\")\\\n",
    ".option(\"truncate\",\"true\")\\\n",
    ".mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-memorabilia",
   "metadata": {},
   "source": [
    "## Combine and insert task 3 to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "conservative-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_task_3 = hasil_pred_3.join(data_username,['customer_id'],'left').select('customer_id','nama','next_time_buy_code','next_time_buy')\n",
    "combined_task_2 = combined_task_2.na.drop(subset=['nama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "stupid-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_task_3.write.format(\"jdbc\")\\\n",
    ".option(\"url\",'jdbc:postgresql://postgres-db-lb.naufalhilmi.svc.cluster.local:5432/postgres')\\\n",
    ".option(\"dbtable\",'public.hasil_task_3')\\\n",
    ".option(\"user\",'postgres')\\\n",
    ".option(\"password\",'postgres')\\\n",
    ".option(\"driver\",\"org.postgresql.Driver\")\\\n",
    ".option(\"truncate\",\"true\")\\\n",
    ".mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-congo",
   "metadata": {},
   "source": [
    "# Spark Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "identified-newman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
